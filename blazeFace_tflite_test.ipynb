{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe99f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blazeface inference\n",
    "\n",
    "# derived from: https://github.com/ibaiGorordo/BlazeFace-TFLite-Inference\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import ImageDraw\n",
    "\n",
    "print(\"TF version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc65d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= #\n",
    "# Configuration #\n",
    "# ============= #\n",
    "\n",
    "ROOT_DIR   = os.path.abspath('')\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'trained_models')\n",
    "\n",
    "config = {\n",
    "    'tfds_dataset_name': 'the300w_lp',\n",
    "    'data_dir': '~/tensorflow_datasets',\n",
    "    'data_train_split_percentage' : 80,\n",
    "    #\n",
    "    'num_test_images': 10,\n",
    "}\n",
    "\n",
    "model_type = 'custom' # 'custom_quant', 'custom', 'front', or 'back'\n",
    "if model_type == 'custom_quant':\n",
    "    model_spec_name =  'bf3'\n",
    "    model_path = os.path.join(OUTPUT_DIR, model_spec_name)\n",
    "    model_spec_name = model_spec_name + '_quant'\n",
    "    config['anchor_strides'] = [8,16]\n",
    "elif model_type == 'custom':\n",
    "    model_spec_name =  'bf3'\n",
    "    model_path = os.path.join(OUTPUT_DIR, model_spec_name)\n",
    "    config['anchor_strides'] = [8,16]\n",
    "elif model_type == 'front':\n",
    "    model_spec_name = 'face_detection_short_range'\n",
    "    model_path = OUTPUT_DIR\n",
    "    config['anchor_strides'] = [8,16]\n",
    "else: # 'back' # not yet working\n",
    "    model_spec_name = 'face_detection_full_range'\n",
    "    model_path = OUTPUT_DIR\n",
    "    config['anchor_strides'] = [16,32]\n",
    "\n",
    "config['model_file'] = os.path.join(model_path, model_spec_name + '.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d173c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== #\n",
    "# Blaze Face Model Class #\n",
    "# ====================== #\n",
    "\n",
    "class blazeFaceDetector():\n",
    "    default_config = {\n",
    "        'scoreThreshold': 0.75, # 0.7\n",
    "        'iouThreshold': 0.5, # 0.3\n",
    "        #\n",
    "        'num_anchors': [2, 6],\n",
    "        'anchor_strides': [8,16],\n",
    "        'num_landmarks': 6,\n",
    "        'max_faces': 100,\n",
    "        #\n",
    "        'detect_coord_order':  'xy', # 'yx'\n",
    "        'delta_normalised_or_absolute': 'absolute', # 'normalised'\n",
    "        'variances': None, # [0.1, 0.1, 0.2, 0.2]\n",
    "        #\n",
    "        'model_file': None, # placeholder to populate from global_config\n",
    "    }\n",
    "\n",
    "    def get_config(self, global_config={}):\n",
    "        config = self.default_config\n",
    "        for key, value in global_config.items():\n",
    "            if key in config and value:\n",
    "                config[key] = value\n",
    "        return config\n",
    "        \n",
    "    def __init__(self, global_config):\n",
    "        self.config = self.get_config(global_config)\n",
    "\n",
    "        self.config['sigmoidScoreThreshold'] = np.log(\n",
    "            self.config['scoreThreshold']/(1-self.config['scoreThreshold']))\n",
    " \n",
    "        self.fps = 0\n",
    "        self.timeLastPrediction = time.time()\n",
    "        self.frameCounter = 0\n",
    "\n",
    "        # Initialize model\n",
    "        self.initializeModel()\n",
    "\n",
    "        # Generate anchors\n",
    "        self.generateAnchors()\n",
    "\n",
    "    class Results:\n",
    "        def __init__(self, boxes, keypoints, scores):\n",
    "            self.boxes = boxes\n",
    "            self.keypoints = keypoints\n",
    "            self.scores = scores\n",
    "        \n",
    "    class Anchor:\n",
    "        def __init__(self, x, y, w=1, h=1):\n",
    "            self.x_center = x\n",
    "            self.y_center = y\n",
    "            self.width = w\n",
    "            self.height = h\n",
    "\n",
    "        def to_string(self):\n",
    "            anchor_s = 'x_center: {:}, y_center: {:}, w: {:}, h: {:}'.format(\n",
    "                self.x_center, self.y_center, self.width, self.height)\n",
    "            \n",
    "            return anchor_s\n",
    "    \n",
    "    def generateAnchors(self):\n",
    "        anchors = []\n",
    "        \n",
    "        for i, stride in enumerate(self.config['anchor_strides']):\n",
    "            num_rows = math.floor((self.inputHeight + stride - 1) / stride)\n",
    "            num_cols = math.floor((self.inputWidth  + stride - 1) / stride)\n",
    "            \n",
    "            num_anchors = self.config['num_anchors'][i]\n",
    "            for y in range(num_rows):\n",
    "                anchor_y = (stride * (y + 0.5)) / self.inputHeight\n",
    "                for x in range(num_cols):\n",
    "                    anchor_x = (stride * (x + 0.5)) / self.inputWidth\n",
    "                    for n in range(num_anchors):\n",
    "                        anchor = self.Anchor(anchor_x, anchor_y, 1., 1.)\n",
    "                        anchors.append(anchor)\n",
    "\n",
    "        self.anchors = anchors\n",
    "        \n",
    "    def initializeModel(self):\n",
    "        print('model_file:', self.config['model_file'])\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=self.config['model_file'])\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        # Get model info\n",
    "        self.getModelInputOutputDetails()\n",
    "        \n",
    "    def detectFacesRGB(self, input_image, num=0):\n",
    "\n",
    "        # Perform inference on the input tensor\n",
    "        output0, output1 = self.inference(input_image)\n",
    "            \n",
    "        # Figure out which output is the scores and which is the detections\n",
    "        if len(output0.shape)==1 or output0.shape[-1]==1:\n",
    "            # output0 is a single vector of scores, so is scores\n",
    "            pred_scores = output0\n",
    "            pred_deltas = output1\n",
    "        else:\n",
    "            # output0 is not a single vector of scores, so is detections\n",
    "            pred_scores = output1\n",
    "            pred_deltas = output0\n",
    "\n",
    "        # Filter scores based on the detection scores\n",
    "        scores, goodDetectionsIndices = self.filterDetections(pred_scores)\n",
    "        \n",
    "        # Extract information of filtered detections\n",
    "        boxes, keypoints = self.extractDetections(pred_deltas, goodDetectionsIndices)\n",
    "\n",
    "        # Filter results with non-maximum suppression\n",
    "        detectionResults = self.filterWithNonMaxSupression(boxes, keypoints, scores)\n",
    "\n",
    "        # Update fps calculator\n",
    "        self.updateFps()\n",
    "\n",
    "        return detectionResults\n",
    "\n",
    "    def updateFps(self):\n",
    "        updateRate = 1\n",
    "        self.frameCounter += 1\n",
    "\n",
    "        # Every updateRate frames calculate the fps based on the ellapsed time\n",
    "        if self.frameCounter == updateRate:\n",
    "            timeNow = time.time()\n",
    "            ellapsedTime = timeNow - self.timeLastPrediction\n",
    "\n",
    "            self.fps = int(updateRate/ellapsedTime)\n",
    "            self.frameCounter = 0\n",
    "            self.timeLastPrediction = timeNow\n",
    "            \n",
    "            print('ellapsedTime: {:.2f} sec'.format(ellapsedTime))\n",
    "\n",
    "    def getGroundTruthBoxCoords(self, box):\n",
    "        # ground truth boundingBox co-ord order is y1,x1,y2,x2\n",
    "        y1, x1, y2, x2 = box[0], box[1], box[2], box[3]\n",
    "        \n",
    "        return x1, y1, x2, y2\n",
    "    \n",
    "    def getBoundingBoxCoords(self, box):\n",
    "        # detection boundingBox co-ord order is x1,y1,x2,y2\n",
    "        x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
    "        \n",
    "        return x1, y1, x2, y2\n",
    "    \n",
    "    def drawGroundTruthRGB(self, img, results):\n",
    "        boundingBoxes = results.boxes\n",
    "        keypoints = results.keypoints\n",
    "\n",
    "        image = img.copy()\n",
    "        img_width, img_height = image.size\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        color = (18, 202, 214, 255) # RGB\n",
    "\n",
    "        # Add bounding boxes and keypoints\n",
    "        for boundingBox, keypoints in zip(boundingBoxes, keypoints):\n",
    "            x1, y1, x2, y2 = self.getGroundTruthBoxCoords(boundingBox)\n",
    "            x1 = (x1 * img_width).astype(int)\n",
    "            x2 = (x2 * img_width).astype(int)\n",
    "            y1 = (y1 * img_height).astype(int)\n",
    "            y2 = (y2 * img_height).astype(int)\n",
    "            box_width = x2 - x1\n",
    "            box_height = y2 - y1\n",
    "            if box_width <= 0 or box_height <= 0:\n",
    "                continue\n",
    "            draw.rectangle((x1, y1, x2, y2), outline=color, width=2)\n",
    "\n",
    "            # Add keypoints for the current face\n",
    "            for keypoint in keypoints:\n",
    "                xKeypoint = (keypoint[0] * img_width).astype(int)\n",
    "                yKeypoint = (keypoint[1] * img_height).astype(int)\n",
    "                draw.ellipse((xKeypoint-2, yKeypoint-2,\n",
    "                              xKeypoint+2, yKeypoint+2),\n",
    "                             fill=color)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def drawDetectionsRGB(self, img, results):\n",
    "        boundingBoxes = results.boxes\n",
    "        keypoints = results.keypoints\n",
    "        scores = results.scores\n",
    "\n",
    "        image = img.copy()\n",
    "        width, height = image.size\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        color = (255, 0, 0, 255)\n",
    "        fnt = None\n",
    "\n",
    "        # Add bounding boxes and keypoints\n",
    "        for boundingBox, keypoints, score in zip(boundingBoxes, keypoints, scores):\n",
    "            x1, y1, x2, y2 = self.getBoundingBoxCoords(boundingBox)\n",
    "            x1 = (x1 * self.inputWidth).astype(int)\n",
    "            x2 = (x2 * self.inputWidth).astype(int)\n",
    "            y1 = (y1 * self.inputHeight).astype(int)\n",
    "            y2 = (y2 * self.inputHeight).astype(int)\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            if width <= 0 or height <= 0:\n",
    "                continue\n",
    "            draw.rectangle((x1, y1, x2, y2), outline=color, width=1)\n",
    "            draw.text((x1, y1 - 10), '{:.2f}'.format(score),\n",
    "                      font=fnt, fill=color)\n",
    "            \n",
    "            # Add keypoints for the current face\n",
    "            for keypoint in keypoints:\n",
    "                xKeypoint = (keypoint[0] * self.inputWidth).astype(int)\n",
    "                yKeypoint = (keypoint[1] * self.inputHeight).astype(int)\n",
    "                draw.ellipse((xKeypoint-2, yKeypoint-2,\n",
    "                              xKeypoint+2, yKeypoint+2),\n",
    "                             fill=color)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def getModelInputOutputDetails(self):\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        \n",
    "        input_shape = self.input_details[0]['shape']\n",
    "        self.inputBatch = input_shape[0]\n",
    "        self.inputHeight = input_shape[1]\n",
    "        self.inputWidth = input_shape[2]\n",
    "        self.inputChannels = input_shape[3]\n",
    "        self.inputDataType = self.input_details[0]['dtype']\n",
    "\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "    # e.g. {'scales': array([0.16255273], dtype=float32), 'zero_points': array([14], dtype=int32), 'quantized_dimension': 0}\n",
    "    # https://www.tensorflow.org/lite/performance/quantization_spec\n",
    "    def dequant(self, quant_tensor, quant_params):\n",
    "        zero_points = quant_params.get('zero_points', 0)\n",
    "        scales = quant_params.get('scales', 1)\n",
    "        dequant_tensor = (quant_tensor - zero_points) * scales\n",
    "        return dequant_tensor\n",
    "\n",
    "    def inference(self, input_tensor):\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor)\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        # get output tensors\n",
    "        output0 = np.squeeze(self.interpreter.get_tensor(self.output_details[0]['index']))\n",
    "        output1 = np.squeeze(self.interpreter.get_tensor(self.output_details[1]['index']))\n",
    "\n",
    "        # dequantize if needed\n",
    "        if self.inputDataType == tf.int8:\n",
    "            output0 = self.dequant(output0, self.output_details[0]['quantization_parameters'])\n",
    "            output1 = self.dequant(output1, self.output_details[1]['quantization_parameters'])\n",
    "\n",
    "        return output0, output1\n",
    "\n",
    "    def getDetectionBoxCoords(self, box, index):\n",
    "        if self.config['detect_coord_order'] == 'yx':\n",
    "            iy, ix, ih, iw = 0, 1, 2, 3\n",
    "        else:\n",
    "            ix, iy, iw, ih = 0, 1, 2, 3\n",
    "\n",
    "        sx = box[index, ix]\n",
    "        sy = box[index, iy]\n",
    "        w  = box[index, iw]\n",
    "        h  = box[index, ih]\n",
    "\n",
    "        return sx, sy, w, h\n",
    "    \n",
    "    def getKeypointCoords(self, box_and_landmarks, index, keypoint_num):\n",
    "        x = box_and_landmarks[index, 4 + (2*keypoint_num) + 0]\n",
    "        y = box_and_landmarks[index, 4 + (2*keypoint_num) + 1]\n",
    "        return x, y\n",
    "    \n",
    "    def extractDetections(self, pred_deltas, goodDetectionsIndices):\n",
    "\n",
    "        numGoodDetections = goodDetectionsIndices.shape[0]\n",
    "        \n",
    "        if not self.config['variances'] is None:\n",
    "            landmark_variances = self.config['num_landmarks'] * self.config['variances'][0:2]\n",
    "            total_variances = self.config['variances'] + landmark_variances\n",
    "            pred_deltas *= total_variances\n",
    "\n",
    "        keypoints = np.zeros((numGoodDetections, self.config['num_landmarks'], 2))\n",
    "        boxes = np.zeros((numGoodDetections, 4))\n",
    "        for idx, detectionIdx in enumerate(goodDetectionsIndices):\n",
    "            anchor = self.anchors[detectionIdx]\n",
    "            sx, sy, w, h = self.getDetectionBoxCoords(pred_deltas, detectionIdx)\n",
    "\n",
    "            # Original custom model used normalised delta\n",
    "            # MediaPipe model uses absolute delta\n",
    "            if self.config['delta_normalised_or_absolute'] == 'normalised':\n",
    "                cx = (sx * anchor.width)  + anchor.x_center\n",
    "                cy = (sy * anchor.height) + anchor.y_center\n",
    "                w  = (w * anchor.width)\n",
    "                h  = (h * anchor.height)\n",
    "            else: # absolute delta, so re-normalise\n",
    "                cx = (sx * anchor.width)  + anchor.x_center * self.inputWidth\n",
    "                cy = (sy * anchor.height) + anchor.y_center * self.inputHeight\n",
    "                w  = (w * anchor.width)\n",
    "                h  = (h * anchor.height)\n",
    "\n",
    "                cx /= self.inputWidth\n",
    "                cy /= self.inputHeight\n",
    "                w  /= self.inputWidth\n",
    "                h  /= self.inputHeight\n",
    "\n",
    "            for j in range(self.config['num_landmarks']):\n",
    "                lx, ly = self.getKeypointCoords(pred_deltas, detectionIdx, j)\n",
    "                if self.config['delta_normalised_or_absolute'] == 'normalised':\n",
    "                    lx = (lx * anchor.width)  + anchor.x_center\n",
    "                    ly = (ly * anchor.height) + anchor.y_center\n",
    "                else: # absolute delta, so re-normalise\n",
    "                    lx = (lx * anchor.width)  + anchor.x_center * self.inputWidth\n",
    "                    ly = (ly * anchor.height) + anchor.y_center * self.inputHeight\n",
    "                    lx /= self.inputWidth\n",
    "                    ly /= self.inputHeight\n",
    "                keypoints[idx,j,:] = np.array([lx, ly])\n",
    "                \n",
    "            boxes[idx,:] = np.array([cx - w * 0.5, cy - h * 0.5, cx + w * 0.5, cy + h * 0.5])\n",
    "        return boxes, keypoints\n",
    "\n",
    "    def filterDetections(self, pred_scores):\n",
    "        # Filter based on the score threshold before applying sigmoid function\n",
    "        goodDetections = np.where(pred_scores > self.config['sigmoidScoreThreshold'])[0]\n",
    "\n",
    "        # Convert scores back from sigmoid values\n",
    "        scores = 1.0 /(1.0 + np.exp(-pred_scores[goodDetections]))\n",
    "\n",
    "        return scores, goodDetections\n",
    "\n",
    "    def filterWithNonMaxSupression(self, boxes, keypoints, scores):\n",
    "        # Filter based on non max suppression\n",
    "        selected_indices = tf.image.non_max_suppression(boxes, scores, self.config['max_faces'], self.config['iouThreshold'])\n",
    "        filtered_boxes = tf.gather(boxes, selected_indices).numpy()\n",
    "        filtered_keypoints = tf.gather(keypoints, selected_indices).numpy()\n",
    "        filtered_scores = tf.gather(scores, selected_indices).numpy()\n",
    "\n",
    "        detectionResults = self.Results(filtered_boxes, filtered_keypoints, filtered_scores)\n",
    "        return detectionResults\n",
    "\n",
    "# Initialize face detector\n",
    "faceDetector = blazeFaceDetector(config)\n",
    "img_size = faceDetector.inputHeight\n",
    "input_data_type = faceDetector.inputDataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ #\n",
    "# Prepare data #\n",
    "# ============ #\n",
    "\n",
    "# reduce total landmarks in data-set to just what is needed for blazeface\n",
    "def filter_landmarks(landmarks):\n",
    "    # Left eye\n",
    "    left_eye_coords = tf.reduce_mean(landmarks[..., 36:42, :], -2)\n",
    "    # Right eye\n",
    "    right_eye_coords = tf.reduce_mean(landmarks[..., 42:48, :], -2)\n",
    "    # Left ear\n",
    "    left_ear_coords = tf.reduce_mean(landmarks[..., 0:2, :], -2)\n",
    "    # Right ear\n",
    "    right_ear_coords = tf.reduce_mean(landmarks[..., 15:17, :], -2)\n",
    "    # Nose\n",
    "    nose_coords = tf.reduce_mean(landmarks[..., 27:36, :], -2)\n",
    "    # Mouth\n",
    "    mouth_coords = tf.reduce_mean(landmarks[..., 48:68, :], -2)\n",
    "    return tf.stack([\n",
    "        left_eye_coords,\n",
    "        right_eye_coords,\n",
    "        left_ear_coords,\n",
    "        right_ear_coords,\n",
    "        nose_coords,\n",
    "        mouth_coords,\n",
    "    ], -2)\n",
    "\n",
    "def generate_bboxes_from_landmarks(landmarks):\n",
    "    ix, iy = 0, 1\n",
    "    padding = 5e-3\n",
    "    x1 = tf.reduce_min(landmarks[..., ix], -1) - padding\n",
    "    x2 = tf.reduce_max(landmarks[..., ix], -1) + padding\n",
    "    y1 = tf.reduce_min(landmarks[..., iy], -1) - padding\n",
    "    y2 = tf.reduce_max(landmarks[..., iy], -1) + padding\n",
    "\n",
    "    gt_boxes = tf.stack([y1, x1, y2, x2], -1)\n",
    "    return tf.clip_by_value(gt_boxes, 0, 1)\n",
    "\n",
    "def preprocessing(image_data, final_height, final_width, data_type=tf.float32):\n",
    "    img = image_data[\"image\"]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    gt_landmarks = tf.expand_dims(image_data[\"landmarks_2d\"], 0)\n",
    "    gt_boxes = generate_bboxes_from_landmarks(gt_landmarks)\n",
    "    gt_landmarks = filter_landmarks(gt_landmarks)\n",
    "    img = tf.image.resize(img, (final_height, final_width))\n",
    "    img = tf.image.convert_image_dtype(img, data_type)\n",
    "    if data_type == tf.float32:\n",
    "        img = (img - 0.5) / 0.5\n",
    "    return img, gt_boxes, gt_landmarks\n",
    "\n",
    "# Prepare data\n",
    "\n",
    "print('Loading tfds dataset:', config['tfds_dataset_name'])\n",
    "rest_split  = 'train[{}%:]'.format(config['data_train_split_percentage'])\n",
    "rest_data, info     = tfds.load(config['tfds_dataset_name'], split=rest_split,  data_dir=config['data_dir'], with_info=True)\n",
    "\n",
    "rest_total_items = info.splits[rest_split].num_examples\n",
    "\n",
    "val_total_items  = int(rest_total_items/2)\n",
    "test_total_items = rest_total_items - val_total_items\n",
    "\n",
    "test_data = rest_data.skip(val_total_items).take(test_total_items)\n",
    "\n",
    "test_data = test_data.map(lambda x : preprocessing(x, img_size, img_size, input_data_type))\n",
    "\n",
    "# (images, ground truth boxes, ground truth landmarks)\n",
    "data_shapes = ([None, None, None], [None, None], [None, None, None])\n",
    "\n",
    "# (images, ground truth boxes, ground truth landmarks)\n",
    "padding_values = (tf.constant(0, input_data_type), tf.constant(0, tf.float32), tf.constant(0, tf.float32))\n",
    "\n",
    "test_data = test_data.padded_batch(1, padded_shapes=data_shapes, padding_values=padding_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af822215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== #\n",
    "# Inference tests #\n",
    "# =============== #\n",
    "\n",
    "def singleImageTensorToPil(single_img_tensor):\n",
    "    single_img_tensor = tf.image.convert_image_dtype(single_img_tensor, tf.float32)\n",
    "    img_pil = tf.keras.preprocessing.image.array_to_img(single_img_tensor)\n",
    "    return img_pil\n",
    "\n",
    "for num, image_data in enumerate(test_data.take(config['num_test_images'])):\n",
    "\n",
    "    # Get test image\n",
    "    input_image, shapes, values = image_data\n",
    "\n",
    "    groundTruthResults = faceDetector.Results(boxes=shapes[0].numpy(), keypoints=values[0].numpy(), scores=None)\n",
    "        \n",
    "    # Detect faces\n",
    "    detectionResults = faceDetector.detectFacesRGB(input_image, num=num)\n",
    "\n",
    "    # Draw detections and ground truth\n",
    "    img_input_pil  = singleImageTensorToPil(input_image[0])\n",
    "    img_detect_pil = faceDetector.drawDetectionsRGB(img_input_pil, detectionResults)\n",
    "    img_truth_pil  = faceDetector.drawGroundTruthRGB(img_input_pil, groundTruthResults)\n",
    "    \n",
    "    width=10\n",
    "    height=10\n",
    "    fig, axarr = plt.subplots(1, 3, figsize=(width,height))\n",
    "\n",
    "    axarr[0].set_title('detection')\n",
    "    axarr[0].imshow(img_detect_pil)\n",
    "\n",
    "    axarr[1].set_title('ground truth')\n",
    "    axarr[1].imshow(img_truth_pil)\n",
    "    \n",
    "    axarr[2].set_title('input image')\n",
    "    axarr[2].imshow(img_input_pil)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
