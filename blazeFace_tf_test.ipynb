{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ea470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "# BlazeFace tensorflow #\n",
    "# - train and test     #\n",
    "# ==================== #\n",
    "\n",
    "# inspired by:\n",
    "# https://github.com/FurkanOM/tf-blazeface.git\n",
    "# https://github.com/PureHing/face-mask-detection-tf2\n",
    "# https://github.com/ibaiGorordo/BlazeFace-TFLite-Inference\n",
    "\n",
    "# BlazeFace paper:\n",
    "# https://arxiv.org/pdf/1907.05047.pdf\n",
    "\n",
    "# Val_conf_loss was always 0 for fixed anchor size\n",
    "# (which Google use for their MediaPipe blaceface model)\n",
    "# iou_map didn't exceed the threshold\n",
    "# Calculation of the 'actual_labels' (delta between ground_truth and anchors),\n",
    "# comes out with empty array\n",
    "# Reduced iou_threhold from 0.5 to 0.3 for fixed anchor size\n",
    "\n",
    "# Note about coordinate systems:\n",
    "# tensorflow bounding boxes use [y_min, x_min, y_max, x_max]\n",
    "# therefore the following here use same [y_min, x_min, y_max, x_max]\n",
    "# - bboxes\n",
    "# - min_max\n",
    "# the following are always [x, y] order:\n",
    "# - landmarks\n",
    "# - anchors (prior_boxes)\n",
    "# detection delta is configurable via config['detect_coord_order']\n",
    "# - Training:  get_deltas_from_bboxes_and_landmarks\n",
    "# - Inference: get_bboxes_and_landmarks_from_deltas\n",
    "\n",
    "# started adding signatures for model export, but not finished yet\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__.startswith(\"2\")\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import ImageDraw\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "\n",
    "# Limit tensorflow log verbosity\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(\"ERROR\")\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= #\n",
    "# Configuration #\n",
    "# ============= #\n",
    "\n",
    "PLOT_MODEL = False\n",
    "LOAD_SAVED_WEIGHTS = True\n",
    "EXPORT_SAVED_MODEL = True\n",
    "EXPORT_TFLITE_MODEL = True\n",
    "EXPORT_QUANTIZED_MODEL = True\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "\n",
    "config = {\n",
    "    \"model_spec_name\": \"bf3\",\n",
    "    \"output_dir\": os.path.join(ROOT_DIR, \"trained_models\"),\n",
    "    \"tfds_dataset_name\": \"the300w_lp\",\n",
    "    \"data_dir\": \"~/tensorflow_datasets\",\n",
    "    \"data_train_split_percentage\": 80,\n",
    "    #\n",
    "    # network settings\n",
    "    \"input_size\": (128, 128),  # (height, width)\n",
    "    \"batch_size\": 32,\n",
    "    \"detections_per_layer\": [6, 2],\n",
    "    \"total_landmarks\": 6,\n",
    "    \"neg_pos_ratio\": 3,\n",
    "    \"loc_loss_alpha\": 1,\n",
    "    \"iou_threshold\": 0.3,  # 0.5,\n",
    "    # 'variances': [0.1, 0.1, 0.2, 0.2], # original\n",
    "    \"variances\": [1.0, 1.0, 1.0, 1.0],  # MediaPipe\n",
    "    # 'detect_coord_order':   'yx', # original\n",
    "    \"detect_coord_order\": \"xy\",  # MediaPipe\n",
    "    # 'delta_normalised_or_absolute': 'normalised', # original\n",
    "    \"delta_normalised_or_absolute\": \"absolute\",  # MediaPipe\n",
    "    #\n",
    "    #     anchor settings\n",
    "    \"clip\": False,\n",
    "    \"min_scale\": 0.1484375,\n",
    "    \"max_scale\": 0.75,\n",
    "    \"feature_map_width\": [],\n",
    "    \"feature_map_height\": [],\n",
    "    \"strides\": [8, 16, 16, 16],\n",
    "    \"aspect_ratios\": [1.0],\n",
    "    \"offset_x\": 0.5,\n",
    "    \"offset_y\": 0.5,\n",
    "    \"reduce_boxes_in_lowest_layer\": False,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": True,  # True for BlazeFace\n",
    "    #\n",
    "    # training settings\n",
    "    # 'epochs':               150, # 3 # 20 #150\n",
    "    # 'limited_steps':        None, # None, or optionally shorten (eg. 50) for dev\n",
    "    #\n",
    "    # test configuration\n",
    "    \"num_test_images\": 10,\n",
    "}\n",
    "\n",
    "saved_model_path = os.path.join(config[\"output_dir\"],\n",
    "                                config[\"model_spec_name\"])\n",
    "model_plot_file = os.path.join(saved_model_path,\n",
    "                               config[\"model_spec_name\"] + \".png\")\n",
    "weights_file = os.path.join(saved_model_path,\n",
    "                            config[\"model_spec_name\"] + \".h5\")\n",
    "tflite_file = os.path.join(saved_model_path,\n",
    "                           config[\"model_spec_name\"] + \".tflite\")\n",
    "tflite_quant_file = os.path.join(saved_model_path,\n",
    "                                 config[\"model_spec_name\"] + \"_quant.tflite\")\n",
    "\n",
    "# if not os.path.exists(config['output_dir']):\n",
    "#     os.mkdir(config['output_dir'])\n",
    "# if not os.path.exists(saved_model_path):\n",
    "#     os.mkdir(saved_model_path)\n",
    "\n",
    "# log_path = \"logs/{}/{}\".format(\n",
    "#      config['model_spec_name'], datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# log_path_simple = \"logs/{}\".format(config['model_spec_name'])\n",
    "# print('created log file:', log_path_simple)\n",
    "# w = tf.summary.create_file_writer(log_path_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= #\n",
    "# Anchor generation #\n",
    "# ================= #\n",
    "\n",
    "\n",
    "class SsdAnchorsCalculatorOptions:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_width,\n",
    "        input_size_height,\n",
    "        min_scale,\n",
    "        max_scale,\n",
    "        num_layers,\n",
    "        feature_map_width,\n",
    "        feature_map_height,\n",
    "        strides,\n",
    "        aspect_ratios,\n",
    "        anchor_offset_x=0.5,\n",
    "        anchor_offset_y=0.5,\n",
    "        reduce_boxes_in_lowest_layer=False,\n",
    "        interpolated_scale_aspect_ratio=1.0,\n",
    "        fixed_anchor_size=False,\n",
    "    ):\n",
    "        # Size of input images.\n",
    "        self.input_size_width = input_size_width\n",
    "        self.input_size_height = input_size_height\n",
    "        # Min and max scales for generating anchor boxes on feature maps.\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        # Offset for the center of anchors. Value is in the scale of stride.\n",
    "        # E.g. 0.5 meaning 0.5 * |current_stride| in pixels.\n",
    "        self.anchor_offset_x = anchor_offset_x\n",
    "        self.anchor_offset_y = anchor_offset_y\n",
    "        # Number of output feature maps to generate the anchors on.\n",
    "        self.num_layers = num_layers\n",
    "        # Sizes of output feature maps to create anchors.\n",
    "        # Either feature_map size or stride should be provided.\n",
    "        self.feature_map_width = feature_map_width\n",
    "        self.feature_map_height = feature_map_height\n",
    "        self.feature_map_width_size = len(feature_map_width)\n",
    "        self.feature_map_height_size = len(feature_map_height)\n",
    "        # Strides of each output feature maps.\n",
    "        self.strides = strides\n",
    "        self.strides_size = len(strides)\n",
    "        # List of different aspect ratio to generate anchors.\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.aspect_ratios_size = len(aspect_ratios)\n",
    "        # A boolean to indicate whether the fixed 3 boxes per location is used\n",
    "        # in the lowest layer.\n",
    "        self.reduce_boxes_in_lowest_layer = reduce_boxes_in_lowest_layer\n",
    "        # An additional anchor is added with this aspect ratio and a scale\n",
    "        # interpolated between the scale for a layer and the scale for the\n",
    "        # next layer (1.0 for the last layer). This anchor is not included\n",
    "        # if this value is 0.\n",
    "        self.interpolated_scale_aspect_ratio = interpolated_scale_aspect_ratio\n",
    "        # Whether use fixed width and height (e.g. both 1.0f) for each anchor.\n",
    "        # This option can be used when the predicted anchor width and height\n",
    "        # are in  pixels.\n",
    "        self.fixed_anchor_size = fixed_anchor_size\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"input_size_width: {:}\\ninput_size_height: {:}\" \\\n",
    "               \"\\nmin_scale: {:}\\nmax_scale: {:}\\nanchor_offset_x: {:}\" \\\n",
    "               \"\\nanchor_offset_y: {:}\\nnum_layers: {:}\" \\\n",
    "               \"\\nfeature_map_width: {:}\\nfeature_map_height: {:}\" \\\n",
    "               \"\\nstrides: {:}\\naspect_ratios: {:}\" \\\n",
    "               \"\\nreduce_boxes_in_lowest_layer: {:}\" \\\n",
    "               \"\\ninterpolated_scale_aspect_ratio: {:}\" \\\n",
    "               \"\\nfixed_anchor_size: {:}\".format(\n",
    "            self.input_size_width,\n",
    "            self.input_size_height,\n",
    "            self.min_scale,\n",
    "            self.max_scale,\n",
    "            self.anchor_offset_x,\n",
    "            self.anchor_offset_y,\n",
    "            self.num_layers,\n",
    "            self.feature_map_width,\n",
    "            self.feature_map_height,\n",
    "            self.strides,\n",
    "            self.aspect_ratios,\n",
    "            self.reduce_boxes_in_lowest_layer,\n",
    "            self.interpolated_scale_aspect_ratio,\n",
    "            self.fixed_anchor_size,\n",
    "        )\n",
    "\n",
    "\n",
    "def gen_anchors(options):\n",
    "    anchors = []\n",
    "    # Verify the options.\n",
    "    if options.strides_size != options.num_layers:\n",
    "        print(\"strides_size and num_layers must be equal.\")\n",
    "        return []\n",
    "\n",
    "    layer_id = 0\n",
    "    while layer_id < options.strides_size:\n",
    "        anchor_height = []\n",
    "        anchor_width = []\n",
    "        aspect_ratios = []\n",
    "        scales = []\n",
    "\n",
    "        # For same strides, we merge the anchors in the same order.\n",
    "        last_same_stride_layer = layer_id\n",
    "        while (\n",
    "            last_same_stride_layer < options.strides_size\n",
    "            and options.strides[last_same_stride_layer] == options.strides[layer_id]\n",
    "        ):\n",
    "            scale = options.min_scale + (\n",
    "                options.max_scale - options.min_scale\n",
    "            ) * 1.0 * last_same_stride_layer / (options.strides_size - 1.0)\n",
    "            if last_same_stride_layer == 0 and options.reduce_boxes_in_lowest_layer:\n",
    "                # For first layer, it can be specified to use predefined anchors.\n",
    "                aspect_ratios.append(1.0)\n",
    "                aspect_ratios.append(2.0)\n",
    "                aspect_ratios.append(0.5)\n",
    "                scales.append(0.1)\n",
    "                scales.append(scale)\n",
    "                scales.append(scale)\n",
    "            else:\n",
    "                for aspect_ratio_id in range(options.aspect_ratios_size):\n",
    "                    aspect_ratios.append(options.aspect_ratios[aspect_ratio_id])\n",
    "                    scales.append(scale)\n",
    "\n",
    "                if options.interpolated_scale_aspect_ratio > 0.0:\n",
    "                    scale_next = (\n",
    "                        1.0\n",
    "                        if last_same_stride_layer == options.strides_size - 1\n",
    "                        else options.min_scale\n",
    "                        + (options.max_scale - options.min_scale)\n",
    "                        * 1.0\n",
    "                        * (last_same_stride_layer + 1)\n",
    "                        / (options.strides_size - 1.0)\n",
    "                    )\n",
    "                    scales.append(math.sqrt(scale * scale_next))\n",
    "                    aspect_ratios.append(\n",
    "                        options.interpolated_scale_aspect_ratio)\n",
    "            last_same_stride_layer += 1\n",
    "        for i in range(len(aspect_ratios)):\n",
    "            ratio_sqrts = math.sqrt(aspect_ratios[i])\n",
    "            anchor_height.append(scales[i] / ratio_sqrts)\n",
    "            anchor_width.append(scales[i] * ratio_sqrts)\n",
    "\n",
    "        feature_map_height = 0\n",
    "        feature_map_width = 0\n",
    "        if options.feature_map_height_size > 0:\n",
    "            feature_map_height = options.feature_map_height[layer_id]\n",
    "            feature_map_width = options.feature_map_width[layer_id]\n",
    "        else:\n",
    "            stride = options.strides[layer_id]\n",
    "            feature_map_height = math.ceil(\n",
    "                1.0 * options.input_size_height / stride)\n",
    "            feature_map_width = math.ceil(\n",
    "                1.0 * options.input_size_width / stride)\n",
    "\n",
    "        for y in range(feature_map_height):\n",
    "            for x in range(feature_map_width):\n",
    "                for anchor_id in range(len(anchor_height)):\n",
    "                    # TODO: Support specifying anchor_offset_x, anchor_offset_y.\n",
    "                    x_center = (x + options.anchor_offset_x) * 1.0 / feature_map_width\n",
    "                    y_center = (y + options.anchor_offset_y) * 1.0 / feature_map_height\n",
    "                    w = 0\n",
    "                    h = 0\n",
    "                    if options.fixed_anchor_size:\n",
    "                        w = 1.0\n",
    "                        h = 1.0\n",
    "                    else:\n",
    "                        w = anchor_width[anchor_id]\n",
    "                        h = anchor_height[anchor_id]\n",
    "                    new_anchor = [x_center, y_center, h, w]\n",
    "                    anchors.append([new_anchor])\n",
    "\n",
    "        layer_id = last_same_stride_layer\n",
    "    return anchors\n",
    "\n",
    "\n",
    "# Configuration options for SSD anchor generation\n",
    "ssd_anchors_calculator_options = SsdAnchorsCalculatorOptions(\n",
    "    input_size_width=config[\"input_size\"][1],\n",
    "    input_size_height=config[\"input_size\"][0],\n",
    "    min_scale=config[\"min_scale\"],\n",
    "    max_scale=config[\"max_scale\"],\n",
    "    anchor_offset_x=config[\"offset_x\"],\n",
    "    anchor_offset_y=config[\"offset_y\"],\n",
    "    num_layers=4,\n",
    "    feature_map_width=[],\n",
    "    feature_map_height=[],\n",
    "    strides=config[\"strides\"],\n",
    "    aspect_ratios=config[\"aspect_ratios\"],\n",
    "    reduce_boxes_in_lowest_layer=config[\"reduce_boxes_in_lowest_layer\"],\n",
    "    interpolated_scale_aspect_ratio=config[\"interpolated_scale_aspect_ratio\"],\n",
    "    fixed_anchor_size=config[\"fixed_anchor_size\"],\n",
    ")\n",
    "\n",
    "print(\"Generating priors boxes (anchors)\")\n",
    "prior_boxes = gen_anchors(ssd_anchors_calculator_options)\n",
    "prior_boxes = tf.concat(prior_boxes, axis=0)\n",
    "prior_boxes = tf.clip_by_value(prior_boxes, 0, 1)\n",
    "\n",
    "print(f\"Prior boxes number:{len(prior_boxes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== #\n",
    "# Landmarks functions #\n",
    "# =================== #\n",
    "\n",
    "\n",
    "def renormalize_landmarks_with_min_max(landmarks, min_max):\n",
    "    \"\"\"Renormalizing given bounding boxes to the new boundaries.\n",
    "    r = (x - min) / (max - min)\n",
    "    outputs:\n",
    "        landmarks = (total_count, total_landmarks, [x, y])\n",
    "        min_max = ([y_min, x_min, y_max, x_max])\n",
    "    \"\"\"\n",
    "    y_min, x_min, y_max, x_max = tf.split(min_max, 4)\n",
    "    renomalized_landmarks = landmarks - tf.concat([x_min, y_min], -1)\n",
    "    renomalized_landmarks /= tf.concat([x_max - x_min, y_max - y_min], -1)\n",
    "    return tf.clip_by_value(renomalized_landmarks, 0, 1)\n",
    "\n",
    "\n",
    "def normalize_landmarks(landmarks, height, width):\n",
    "    \"\"\"Normalizing landmarks.\n",
    "    inputs:\n",
    "        landmarks = (M, N, [x, y])\n",
    "        height = image height\n",
    "        width = image width\n",
    "\n",
    "    outputs:\n",
    "        normalized_landmarks = (M, N, [x, y])\n",
    "            in normalized form [0, 1]\n",
    "    \"\"\"\n",
    "    return landmarks / tf.cast([width, height], tf.float32)\n",
    "\n",
    "\n",
    "def denormalize_landmarks(landmarks, height, width):\n",
    "    \"\"\"Denormalizing landmarks.\n",
    "    inputs:\n",
    "        landmarks = (M, N, [x, y])\n",
    "            in normalized form [0, 1]\n",
    "        height = image height\n",
    "        width = image width\n",
    "\n",
    "    outputs:\n",
    "        denormalized_landmarks = (M, N, [x, y])\n",
    "    \"\"\"\n",
    "    return tf.round(landmarks * tf.cast([width, height], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48597c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== #\n",
    "# Bounding box functions #\n",
    "# ====================== #\n",
    "\n",
    "\n",
    "def get_weighted_boxes_and_landmarks(scores, bboxes_and_landmarks, mask):\n",
    "    \"\"\"Calculating weighted mean of given bboxes and landmarks according to\n",
    "    the mask.\n",
    "    inputs:\n",
    "        scores = (total_bboxes, [probability])\n",
    "        bboxes_and_landmarks = (total_bboxes, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "        mask = (total_bboxes,)\n",
    "\n",
    "    outputs:\n",
    "        weighted_bbox_and_landmark = (1, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "    \"\"\"\n",
    "    selected_scores = scores[mask]\n",
    "    selected_bboxes_and_landmarks = bboxes_and_landmarks[mask]\n",
    "    weighted_sum = tf.reduce_sum(\n",
    "        selected_bboxes_and_landmarks * selected_scores, 0)\n",
    "    sum_selected_scores = tf.reduce_sum(selected_scores, 0)\n",
    "    sum_selected_scores = tf.where(\n",
    "        tf.equal(sum_selected_scores, 0.0), 1.0, sum_selected_scores\n",
    "    )\n",
    "    return tf.expand_dims(weighted_sum / sum_selected_scores, 0)\n",
    "\n",
    "\n",
    "def weighted_suppression_body(\n",
    "    counter, iou_threshold, scores, bboxes_and_landmarks,\n",
    "    weighted_suppressed_data\n",
    "):\n",
    "    \"\"\"Weighted mean suppression algorithm while body.\n",
    "    inputs:\n",
    "        counter = while body counter\n",
    "        iou_threshold = threshold value for overlapping bounding boxes\n",
    "        scores = (total_bboxes, [probability])\n",
    "        bboxes_and_landmarks = (total_bboxes, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "        weighted_suppressed_data = (M, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "\n",
    "    outputs:\n",
    "        counter = while body counter\n",
    "        iou_threshold = threshold value for overlapping bounding boxes\n",
    "        scores = (total_bboxes - N, [probability])\n",
    "        bboxes_and_landmarks = (total_bboxes - N, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "        weighted_suppressed_data = (M + 1, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "    \"\"\"\n",
    "    counter = tf.add(counter, 1)\n",
    "    first_box = bboxes_and_landmarks[0, 0:4]\n",
    "    iou_map = generate_iou_map(\n",
    "        first_box, bboxes_and_landmarks[..., 0:4], transpose_perm=[1, 0]\n",
    "    )\n",
    "    overlapped_mask = tf.reshape(tf.greater(iou_map, iou_threshold), (-1,))\n",
    "    weighted_bbox_and_landmark = get_weighted_boxes_and_landmarks(\n",
    "        scores, bboxes_and_landmarks, overlapped_mask\n",
    "    )\n",
    "    weighted_suppressed_data = tf.concat(\n",
    "        [weighted_suppressed_data, weighted_bbox_and_landmark], axis=0\n",
    "    )\n",
    "    not_overlapped_mask = tf.logical_not(overlapped_mask)\n",
    "    scores = scores[not_overlapped_mask]\n",
    "    bboxes_and_landmarks = bboxes_and_landmarks[not_overlapped_mask]\n",
    "    return (\n",
    "        counter,\n",
    "        iou_threshold,\n",
    "        scores,\n",
    "        bboxes_and_landmarks,\n",
    "        weighted_suppressed_data,\n",
    "    )\n",
    "\n",
    "\n",
    "def weighted_suppression(\n",
    "    scores,\n",
    "    bboxes_and_landmarks,\n",
    "    max_total_size=50,\n",
    "    score_threshold=0.75,\n",
    "    iou_threshold=0.3,\n",
    "):\n",
    "    \"\"\"Blazeface weighted mean suppression algorithm.\n",
    "    inputs:\n",
    "        scores = (total_bboxes, [probability])\n",
    "        bboxes_and_landmarks = (total_bboxes, [y1, x1, y2, x2, landmark_x0,\n",
    "        landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "        max_total_size = maximum returned bounding boxes and landmarks\n",
    "        score_threshold = threshold value for bounding boxes and landmarks\n",
    "                          selection\n",
    "        iou_threshold = threshold value for overlapping bounding boxes\n",
    "\n",
    "    outputs:\n",
    "        weighted_bboxes_and_landmarks = (dynamic_size, [y1, x1, y2, x2,\n",
    "        landmark_x0, landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "    \"\"\"\n",
    "    # Filter based on the score threshold before applying sigmoid function\n",
    "    score_mask = tf.squeeze(tf.greater(scores, score_threshold), -1)\n",
    "    scores = scores[score_mask]\n",
    "    # Convert scores back from sigmoid values\n",
    "    scores_sig = tf.math.sigmoid(scores)\n",
    "\n",
    "    bboxes_and_landmarks = bboxes_and_landmarks[score_mask]\n",
    "    sorted_indices = tf.argsort(scores_sig, axis=0, direction=\"DESCENDING\")\n",
    "    sorted_scores = tf.gather_nd(scores_sig, sorted_indices)\n",
    "    sorted_bboxes_and_landmarks = tf.gather_nd(bboxes_and_landmarks,\n",
    "                                               sorted_indices)\n",
    "    counter = tf.constant(0, tf.int32)\n",
    "    weighted_data = tf.zeros(tf.shape(bboxes_and_landmarks[0:1]),\n",
    "                             dtype=tf.float32)\n",
    "    cond = lambda counter, iou_threshold, scores_sig, data, weighted: tf.logical_and(\n",
    "        tf.less(counter, max_total_size),\n",
    "        tf.greater(tf.shape(scores_sig)[0], 0)\n",
    "    )\n",
    "    _, _, _, _, weighted_data = tf.while_loop(\n",
    "        cond,\n",
    "        weighted_suppression_body,\n",
    "        [\n",
    "            counter,\n",
    "            iou_threshold,\n",
    "            sorted_scores,\n",
    "            sorted_bboxes_and_landmarks,\n",
    "            weighted_data,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    weighted_data = weighted_data[1:]\n",
    "    pad_size = max_total_size - weighted_data.shape[0]\n",
    "    weighted_data = tf.pad(weighted_data, ((0, pad_size), (0, 0)))\n",
    "    return weighted_data, sorted_scores\n",
    "\n",
    "\n",
    "def non_max_suppression(pred_bboxes, pred_labels, **kwargs):\n",
    "    \"\"\"Applying non maximum suppression.\n",
    "    Details could be found on tensorflow documentation.\n",
    "    https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression\n",
    "    inputs:\n",
    "        pred_bboxes = (batch_size, total_bboxes, total_labels,\n",
    "                       [y1, x1, y2, x2])\n",
    "            total_labels should be 1 for binary operations like in rpn\n",
    "        pred_labels = (batch_size, total_bboxes, total_labels)\n",
    "        **kwargs = other parameters\n",
    "\n",
    "    outputs:\n",
    "        nms_boxes = (batch_size, max_detections, [y1, x1, y2, x2])\n",
    "        nmsed_scores = (batch_size, max_detections)\n",
    "        nmsed_classes = (batch_size, max_detections)\n",
    "        valid_detections = (batch_size)\n",
    "            Only the top valid_detections[i] entries in nms_boxes[i],\n",
    "            nms_scores[i] and nms_class[i] are valid.\n",
    "            The rest of the entries are zero paddings.\n",
    "    \"\"\"\n",
    "    return tf.image.combined_non_max_suppression(pred_bboxes, pred_labels,\n",
    "                                                 **kwargs)\n",
    "\n",
    "\n",
    "def generate_iou_map(bboxes, gt_boxes, transpose_perm=[0, 2, 1]):\n",
    "    \"\"\"Calculating intersection over union values for each ground truth boxes\n",
    "    in a dynamic manner.\n",
    "    It is supported from 1d to 3d dimensions for bounding boxes.\n",
    "    Even if bboxes have different rank from gt_boxes it should be work.\n",
    "    inputs:\n",
    "        bboxes = (dynamic_dimension, [y1, x1, y2, x2])\n",
    "        gt_boxes = (dynamic_dimension, [y1, x1, y2, x2])\n",
    "        transpose_perm = (transpose_perm_order)\n",
    "            for 3d gt_boxes => [0, 2, 1]\n",
    "\n",
    "    outputs:\n",
    "        iou_map = (dynamic_dimension, total_gt_boxes)\n",
    "            same rank with the gt_boxes\n",
    "    \"\"\"\n",
    "    gt_rank = tf.rank(gt_boxes)\n",
    "    gt_expand_axis = gt_rank - 2\n",
    "\n",
    "    bbox_y1, bbox_x1, bbox_y2, bbox_x2 = tf.split(bboxes, 4, axis=-1)\n",
    "    gt_y1, gt_x1, gt_y2, gt_x2 = tf.split(gt_boxes, 4, axis=-1)\n",
    "\n",
    "    # Calculate bbox and ground truth boxes areas\n",
    "    gt_area = tf.squeeze((gt_y2 - gt_y1) * (gt_x2 - gt_x1), axis=-1)\n",
    "    bbox_area = tf.squeeze((bbox_y2 - bbox_y1) * (bbox_x2 - bbox_x1), axis=-1)\n",
    "\n",
    "    x_top = tf.maximum(bbox_x1, tf.transpose(gt_x1, transpose_perm))\n",
    "    y_top = tf.maximum(bbox_y1, tf.transpose(gt_y1, transpose_perm))\n",
    "    x_bottom = tf.minimum(bbox_x2, tf.transpose(gt_x2, transpose_perm))\n",
    "    y_bottom = tf.minimum(bbox_y2, tf.transpose(gt_y2, transpose_perm))\n",
    "\n",
    "    w_max = tf.maximum(x_bottom - x_top, 0)\n",
    "    h_max = tf.maximum(y_bottom - y_top, 0)\n",
    "\n",
    "    # Calculate intersection area\n",
    "    intersection_area = w_max * h_max\n",
    "    # Calculate union area\n",
    "    union_area = (\n",
    "        tf.expand_dims(bbox_area, -1)\n",
    "        + tf.expand_dims(gt_area, gt_expand_axis)\n",
    "        - intersection_area\n",
    "    )\n",
    "\n",
    "    # Intersection over Union\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "def get_bboxes_and_landmarks_from_deltas(prior_boxes, deltas):\n",
    "    \"\"\"Calculating bounding boxes and landmarks for given delta values.\n",
    "    inputs:\n",
    "        prior_boxes = (total_bboxes, [center_x, center_y, width, height])\n",
    "        deltas = (batch_size, total_bboxes, [delta_bbox_y, delta_bbox_x,\n",
    "        delta_bbox_h, delta_bbox_w, delta_landmark_x0, delta_landmark_y0, ...,\n",
    "        delta_landmark_xN, delta_landmark_yN])\n",
    "\n",
    "    outputs:\n",
    "        bboxes_and_landmarks = (batch_size, total_bboxes, [y1, x1, y2, x2,\n",
    "        landmark_x0, landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "    \"\"\"\n",
    "\n",
    "    if config[\"detect_coord_order\"] == \"xy\":\n",
    "        xi, yi, wi, hi = 0, 1, 2, 3\n",
    "    else:\n",
    "        yi, xi, hi, wi = 0, 1, 2, 3\n",
    "    pxi, pyi, pwi, phi = 0, 1, 2, 3\n",
    "\n",
    "    delta_x = deltas[..., xi]\n",
    "    delta_y = deltas[..., yi]\n",
    "    delta_w = deltas[..., wi]\n",
    "    delta_h = deltas[..., hi]\n",
    "\n",
    "    anchor_x = prior_boxes[..., pxi]\n",
    "    anchor_y = prior_boxes[..., pyi]\n",
    "    anchor_w = prior_boxes[..., pwi]\n",
    "    anchor_h = prior_boxes[..., phi]\n",
    "\n",
    "    image_height, image_width = tf.cast(config[\"input_size\"], tf.float32)\n",
    "\n",
    "    if config[\"delta_normalised_or_absolute\"] == \"absolute\":\n",
    "        bbox_ctr_x = delta_x * anchor_w + anchor_x * image_width\n",
    "        bbox_ctr_y = delta_y * anchor_h + anchor_y * image_height\n",
    "        bbox_width = delta_w * anchor_w\n",
    "        bbox_height = delta_h * anchor_h\n",
    "\n",
    "        bbox_ctr_x /= image_width\n",
    "        bbox_ctr_y /= image_height\n",
    "        bbox_width /= image_width\n",
    "        bbox_height /= image_height\n",
    "    else:  # normalised\n",
    "        bbox_ctr_x = delta_x * anchor_w + anchor_x\n",
    "        bbox_ctr_y = delta_y * anchor_h + anchor_y\n",
    "        bbox_width = delta_w * anchor_w\n",
    "        bbox_height = delta_h * anchor_h\n",
    "\n",
    "    y1 = bbox_ctr_y - (0.5 * bbox_height)\n",
    "    x1 = bbox_ctr_x - (0.5 * bbox_width)\n",
    "    y2 = bbox_height + y1\n",
    "    x2 = bbox_width + x1\n",
    "\n",
    "    total_landmarks = tf.shape(deltas[..., 4:])[-1] // 2\n",
    "    xy_pairs = tf.tile(prior_boxes[..., 0:2], (1, total_landmarks))\n",
    "    wh_pairs = tf.tile(prior_boxes[..., 2:4], (1, total_landmarks))\n",
    "    if config[\"delta_normalised_or_absolute\"] == \"absolute\":\n",
    "        px, py = tf.split(xy_pairs, 2, axis=1)\n",
    "        xy_pairs = tf.concat([px * image_width, py * image_height], -1)\n",
    "        image_wh_pairs = tf.tile([image_width, image_height],\n",
    "                                 [total_landmarks])\n",
    "        landmarks = (deltas[..., 4:] * wh_pairs) + xy_pairs\n",
    "        landmarks /= image_wh_pairs\n",
    "    else:  # normalised\n",
    "        landmarks = (deltas[..., 4:] * wh_pairs) + xy_pairs\n",
    "\n",
    "    bbl = tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "    return tf.concat([bbl, landmarks], -1)\n",
    "\n",
    "\n",
    "def get_deltas_from_bboxes_and_landmarks(prior_boxes, bboxes_and_landmarks):\n",
    "    \"\"\"Calculating bounding box and landmark deltas for given ground truth\n",
    "    boxes and landmarks.\n",
    "    inputs:\n",
    "        prior_boxes = (total_bboxes, [center_x, center_y, width, height])\n",
    "        bboxes_and_landmarks = (batch_size, total_bboxes, [y1, x1, y2, x2,\n",
    "        landmark_x0, landmark_y0, ..., landmark_xN, landmark_yN])\n",
    "\n",
    "    outputs:\n",
    "        deltas = (batch_size, total_bboxes, [delta_bbox_y, delta_bbox_x,\n",
    "        delta_bbox_h, delta_bbox_w, delta_landmark_x0, delta_landmark_y0, ...,\n",
    "        delta_landmark_xN, delta_landmark_yN])\n",
    "    \"\"\"\n",
    "\n",
    "    y1i, x1i, y2i, x2i = 0, 1, 2, 3\n",
    "    pxi, pyi, pwi, phi = 0, 1, 2, 3\n",
    "\n",
    "    gt_width = bboxes_and_landmarks[..., x2i] - bboxes_and_landmarks[..., x1i]\n",
    "    gt_height = bboxes_and_landmarks[..., y2i] - bboxes_and_landmarks[..., y1i]\n",
    "    gt_ctr_x = bboxes_and_landmarks[..., x1i] + 0.5 * gt_width\n",
    "    gt_ctr_y = bboxes_and_landmarks[..., y1i] + 0.5 * gt_height\n",
    "\n",
    "    anchor_x = prior_boxes[..., pxi]\n",
    "    anchor_y = prior_boxes[..., pyi]\n",
    "    anchor_w = prior_boxes[..., pwi]\n",
    "    anchor_h = prior_boxes[..., phi]\n",
    "\n",
    "    delta_x = (gt_ctr_x - anchor_x) / anchor_w\n",
    "    delta_y = (gt_ctr_y - anchor_y) / anchor_h\n",
    "    delta_w = gt_width / anchor_w\n",
    "    delta_h = gt_height / anchor_h\n",
    "\n",
    "    image_height, image_width = tf.cast(config[\"input_size\"], tf.float32)\n",
    "\n",
    "    if config[\"delta_normalised_or_absolute\"] == \"absolute\":\n",
    "        # re-normalise from absolute values before comparison\n",
    "        delta_x = delta_x * image_width\n",
    "        delta_y = delta_y * image_height\n",
    "        delta_w = delta_w * image_width\n",
    "        delta_h = delta_h * image_height\n",
    "\n",
    "    total_landmarks = tf.shape(bboxes_and_landmarks[..., 4:])[-1] // 2\n",
    "    xy_pairs = tf.tile(prior_boxes[..., 0:2], (1, total_landmarks))\n",
    "    wh_pairs = tf.tile(prior_boxes[..., 2:4], (1, total_landmarks))\n",
    "    landmark_deltas = (bboxes_and_landmarks[..., 4:] - xy_pairs) / wh_pairs\n",
    "    if config[\"delta_normalised_or_absolute\"] == \"absolute\":\n",
    "        # re-normalise from absolute values before comparison\n",
    "        image_wh_pairs = tf.tile([image_width, image_height],\n",
    "                                 [total_landmarks])\n",
    "        landmark_deltas *= image_wh_pairs\n",
    "\n",
    "    if config[\"detect_coord_order\"] == \"xy\":\n",
    "        deltas = tf.stack([delta_x, delta_y, delta_w, delta_h], -1)\n",
    "    else:\n",
    "        deltas = tf.stack([delta_y, delta_x, delta_h, delta_w], -1)\n",
    "    return tf.concat([deltas, landmark_deltas], -1)\n",
    "\n",
    "\n",
    "def convert_xywh_to_bboxes(xywh):\n",
    "    \"\"\"Converting center x, y and width height format to bounding boxes.\n",
    "    inputs:\n",
    "        xywh = (M, N, [center_x, center_y, width, height])\n",
    "\n",
    "    outputs:\n",
    "        bboxes = (M, N, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    xi, yi, wi, hi = 0, 1, 2, 3\n",
    "\n",
    "    y1 = xywh[..., yi] - (0.5 * xywh[..., hi])\n",
    "    x1 = xywh[..., xi] - (0.5 * xywh[..., wi])\n",
    "    y2 = xywh[..., hi] + y1\n",
    "    x2 = xywh[..., wi] + x1\n",
    "\n",
    "    bboxes = tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "    return tf.clip_by_value(bboxes, 0, 1)\n",
    "\n",
    "\n",
    "def renormalize_bboxes_with_min_max(bboxes, min_max):\n",
    "    \"\"\"Renormalizing given bounding boxes to the new boundaries.\n",
    "    r = (x - min) / (max - min)\n",
    "    outputs:\n",
    "        bboxes = (total_bboxes, [y1, x1, y2, x2])\n",
    "        min_max = ([y_min, x_min, y_max, x_max])\n",
    "    \"\"\"\n",
    "    y_min, x_min, y_max, x_max = tf.split(min_max, 4)\n",
    "    renomalized_bboxes = bboxes - tf.concat([y_min, x_min, y_min, x_min], -1)\n",
    "    renomalized_bboxes /= tf.concat(\n",
    "        [y_max - y_min, x_max - x_min, y_max - y_min, x_max - x_min], -1\n",
    "    )\n",
    "    return tf.clip_by_value(renomalized_bboxes, 0, 1)\n",
    "\n",
    "\n",
    "def normalize_bboxes(bboxes, height, width):\n",
    "    \"\"\"Normalizing bounding boxes.\n",
    "    inputs:\n",
    "        bboxes = (M, N, [y1, x1, y2, x2])\n",
    "        height = image height\n",
    "        width = image width\n",
    "\n",
    "    outputs:\n",
    "        normalized_bboxes = (M, N, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "    \"\"\"\n",
    "    y1i, x1i, y2i, x2i = 0, 1, 2, 3\n",
    "    y1 = bboxes[..., y1i] / height\n",
    "    x1 = bboxes[..., x1i] / width\n",
    "    y2 = bboxes[..., y2i] / height\n",
    "    x2 = bboxes[..., x2i] / width\n",
    "\n",
    "    box = tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "    return tf.round(box)\n",
    "\n",
    "\n",
    "def denormalize_bboxes(bboxes, height, width):\n",
    "    \"\"\"Denormalizing bounding boxes.\n",
    "    inputs:\n",
    "        bboxes = (M, N, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "        height = image height\n",
    "        width = image width\n",
    "\n",
    "    outputs:\n",
    "        denormalized_bboxes = (M, N, [y1, x1, y2, x2])\n",
    "    \"\"\"\n",
    "    y1i, x1i, y2i, x2i = 0, 1, 2, 3\n",
    "    y1 = bboxes[..., y1i] * height\n",
    "    x1 = bboxes[..., x1i] * width\n",
    "    y2 = bboxes[..., y2i] * height\n",
    "    x2 = bboxes[..., x2i] * width\n",
    "\n",
    "    box = tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "    return tf.round(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedef090-f208-437c-9120-0d76d870911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== #\n",
    "# Data augmentation functions #\n",
    "# =========================== #\n",
    "\n",
    "\n",
    "def apply_augmentation(img, gt_boxes, gt_landmarks):\n",
    "    \"\"\"Randomly applying data augmentation methods to image and ground truth\n",
    "    boxes.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "            in normalized form [0, 1]\n",
    "    outputs:\n",
    "        modified_img = (final_height, final_width, depth)\n",
    "        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "        modified_gt_landmarks = (ground_truth_object_count, total_landmarks,\n",
    "        [x, y])\n",
    "            in normalized form [0, 1]\n",
    "    \"\"\"\n",
    "    # Color operations\n",
    "    # Randomly change hue, saturation, brightness and contrast of image\n",
    "    color_methods = [random_brightness, random_contrast, random_hue,\n",
    "                     random_saturation]\n",
    "    # Geometric operations\n",
    "    # Randomly sample a patch image and ground truth boxes\n",
    "    geometric_methods = [patch]\n",
    "\n",
    "    for augmentation_method in geometric_methods + color_methods:\n",
    "        img, gt_boxes, gt_landmarks = randomly_apply_operation(\n",
    "            augmentation_method, img, gt_boxes, gt_landmarks\n",
    "        )\n",
    "\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    return img, gt_boxes, gt_landmarks\n",
    "\n",
    "\n",
    "def get_random_bool():\n",
    "    \"\"\"Generating random boolean.\n",
    "    outputs:\n",
    "        random boolean 0d tensor\n",
    "    \"\"\"\n",
    "    return tf.greater(tf.random.uniform((), dtype=tf.float32), 0.5)\n",
    "\n",
    "\n",
    "def randomly_apply_operation(operation, img, gt_boxes, gt_landmarks, *args):\n",
    "    \"\"\"Randomly applying given method to image and ground truth boxes.\n",
    "    inputs:\n",
    "        operation = callable method\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    outputs:\n",
    "        modified_or_not_img = (final_height, final_width, depth)\n",
    "        modified_or_not_gt_boxes = (ground_truth_object_count,\n",
    "                                    [y1, x1, y2, x2])\n",
    "        modified_or_not_gt_landmarks = (ground_truth_object_count,\n",
    "        total_landmarks, [x, y])\n",
    "    \"\"\"\n",
    "    return tf.cond(\n",
    "        get_random_bool(),\n",
    "        lambda: operation(img, gt_boxes, gt_landmarks, *args),\n",
    "        lambda: (img, gt_boxes, gt_landmarks),\n",
    "    )\n",
    "\n",
    "\n",
    "def random_brightness(img, gt_boxes, gt_landmarks, max_delta=0.12):\n",
    "    \"\"\"Randomly change brightness of the image.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    outputs:\n",
    "        modified_img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    \"\"\"\n",
    "    return tf.image.random_brightness(img, max_delta), gt_boxes, gt_landmarks\n",
    "\n",
    "\n",
    "def random_contrast(img, gt_boxes, gt_landmarks, lower=0.5, upper=1.5):\n",
    "    \"\"\"Randomly change contrast of the image.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    outputs:\n",
    "        modified_img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    \"\"\"\n",
    "    return tf.image.random_contrast(img, lower, upper), gt_boxes, gt_landmarks\n",
    "\n",
    "\n",
    "def random_hue(img, gt_boxes, gt_landmarks, max_delta=0.08):\n",
    "    \"\"\"Randomly change hue of the image.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    outputs:\n",
    "        modified_img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    \"\"\"\n",
    "    return tf.image.random_hue(img, max_delta), gt_boxes, gt_landmarks\n",
    "\n",
    "\n",
    "def random_saturation(img, gt_boxes, gt_landmarks, lower=0.5, upper=1.5):\n",
    "    \"\"\"Randomly change saturation of the image.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    outputs:\n",
    "        modified_img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "    \"\"\"\n",
    "    return (tf.image.random_saturation(img, lower, upper),\n",
    "            gt_boxes, gt_landmarks)\n",
    "\n",
    "\n",
    "# ================== #\n",
    "# Sample patch start #\n",
    "# ================== #\n",
    "\n",
    "\n",
    "def get_random_min_overlap():\n",
    "    \"\"\"Generating random minimum overlap value.\n",
    "    outputs:\n",
    "        min_overlap = random minimum overlap value 0d tensor\n",
    "    \"\"\"\n",
    "    overlaps = tf.constant([0.1, 0.3, 0.5, 0.7, 0.9], dtype=tf.float32)\n",
    "    i = tf.random.uniform((), minval=0, maxval=tf.shape(overlaps)[0],\n",
    "                          dtype=tf.int32)\n",
    "    return overlaps[i]\n",
    "\n",
    "\n",
    "def expand_image(img, gt_boxes, gt_landmarks, height, width):\n",
    "    \"\"\"Randomly expanding image and adjusting ground truth object coordinates.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "        height = height of the image\n",
    "        width = width of the image\n",
    "    outputs:\n",
    "        modified_img = (final_height, final_width, depth)\n",
    "        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "        modified_gt_landmarks = (ground_truth_object_count, total_landmarks,\n",
    "        [x, y])\n",
    "    \"\"\"\n",
    "    expansion_ratio = tf.random.uniform((), minval=1, maxval=4,\n",
    "                                        dtype=tf.float32)\n",
    "    final_height, final_width = tf.round(height * expansion_ratio), tf.round(\n",
    "        width * expansion_ratio\n",
    "    )\n",
    "    pad_left = tf.round(\n",
    "        tf.random.uniform((), minval=0, maxval=final_width - width,\n",
    "                          dtype=tf.float32)\n",
    "    )\n",
    "    pad_top = tf.round(\n",
    "        tf.random.uniform((), minval=0, maxval=final_height - height,\n",
    "                          dtype=tf.float32)\n",
    "    )\n",
    "    pad_right = final_width - (width + pad_left)\n",
    "    pad_bottom = final_height - (height + pad_top)\n",
    "\n",
    "    mean, _ = tf.nn.moments(img, [0, 1])\n",
    "    expanded_image = tf.pad(\n",
    "        img, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n",
    "        constant_values=-1\n",
    "    )\n",
    "    expanded_image = tf.where(expanded_image == -1, mean, expanded_image)\n",
    "\n",
    "    min_max = tf.stack(\n",
    "        [-pad_top, -pad_left, pad_bottom + height, pad_right + width], -1\n",
    "    ) / [height, width, height, width]\n",
    "    modified_gt_boxes = renormalize_bboxes_with_min_max(gt_boxes, min_max)\n",
    "    modified_gt_landmarks = renormalize_landmarks_with_min_max(gt_landmarks,\n",
    "                                                               min_max)\n",
    "\n",
    "    return expanded_image, modified_gt_boxes, modified_gt_landmarks\n",
    "\n",
    "\n",
    "def patch(img, gt_boxes, gt_landmarks):\n",
    "    \"\"\"Generating random patch and adjusting image and ground truth objects to\n",
    "    this patch. After this operation some of the ground truth boxes / objects\n",
    "    could be removed from the image. However, these objects are not excluded\n",
    "    from the output, only the coordinates are changed as zero.\n",
    "    inputs:\n",
    "        img = (height, width, depth)\n",
    "        gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "        gt_landmarks = (ground_truth_object_count, total_landmarks, [x, y])\n",
    "            in normalized form [0, 1]\n",
    "    outputs:\n",
    "        modified_img = (final_height, final_width, depth)\n",
    "        modified_gt_boxes = (ground_truth_object_count, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "        modified_gt_landmarks = (ground_truth_object_count, total_landmarks,\n",
    "        [x, y])\n",
    "            in normalized form [0, 1]\n",
    "    \"\"\"\n",
    "    img_shape = tf.cast(tf.shape(img), dtype=tf.float32)\n",
    "    org_height, org_width = img_shape[0], img_shape[1]\n",
    "    # Randomly expand image and adjust bounding boxes\n",
    "    img, gt_boxes, gt_landmarks = randomly_apply_operation(\n",
    "        expand_image, img, gt_boxes, gt_landmarks, org_height, org_width\n",
    "    )\n",
    "    # Get random minimum overlap value\n",
    "    min_overlap = get_random_min_overlap()\n",
    "\n",
    "    begin, size, new_boundaries = tf.image.sample_distorted_bounding_box(\n",
    "        tf.shape(img),\n",
    "        bounding_boxes=tf.expand_dims(gt_boxes, 0),\n",
    "        min_object_covered=min_overlap,\n",
    "    )\n",
    "\n",
    "    img = tf.slice(img, begin, size)\n",
    "    img = tf.image.resize(img, (org_height, org_width))\n",
    "    gt_boxes = renormalize_bboxes_with_min_max(gt_boxes, new_boundaries[0, 0])\n",
    "    gt_landmarks = renormalize_landmarks_with_min_max(\n",
    "        gt_landmarks, new_boundaries[0, 0]\n",
    "    )\n",
    "\n",
    "    return img, gt_boxes, gt_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf67a4-c175-4c8c-a739-50d647cee37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== #\n",
    "# Data preparation functions #\n",
    "# ========================== #\n",
    "\n",
    "\n",
    "# reduce total landmarks in data-set to just what is needed for blazeface\n",
    "def filter_landmarks(landmarks):\n",
    "    # Left eye\n",
    "    left_eye_coords = tf.reduce_mean(landmarks[..., 36:42, :], -2)\n",
    "    # Right eye\n",
    "    right_eye_coords = tf.reduce_mean(landmarks[..., 42:48, :], -2)\n",
    "    # Left ear\n",
    "    left_ear_coords = tf.reduce_mean(landmarks[..., 0:2, :], -2)\n",
    "    # Right ear\n",
    "    right_ear_coords = tf.reduce_mean(landmarks[..., 15:17, :], -2)\n",
    "    # Nose\n",
    "    nose_coords = tf.reduce_mean(landmarks[..., 27:36, :], -2)\n",
    "    # Mouth\n",
    "    mouth_coords = tf.reduce_mean(landmarks[..., 48:68, :], -2)\n",
    "    return tf.stack(\n",
    "        [\n",
    "            left_eye_coords,\n",
    "            right_eye_coords,\n",
    "            left_ear_coords,\n",
    "            right_ear_coords,\n",
    "            nose_coords,\n",
    "            mouth_coords,\n",
    "        ],\n",
    "        -2,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_bboxes_from_landmarks(landmarks):\n",
    "    padding = 5e-3\n",
    "    xi, yi = 0, 1\n",
    "    x1 = tf.reduce_min(landmarks[..., xi], -1) - padding\n",
    "    x2 = tf.reduce_max(landmarks[..., xi], -1) + padding\n",
    "    y1 = tf.reduce_min(landmarks[..., yi], -1) - padding\n",
    "    y2 = tf.reduce_max(landmarks[..., yi], -1) + padding\n",
    "\n",
    "    gt_boxes = tf.stack([y1, x1, y2, x2], -1)\n",
    "    return tf.clip_by_value(gt_boxes, 0, 1)\n",
    "\n",
    "\n",
    "def preprocessing(image_data, final_height, final_width, augmentation_fn=None):\n",
    "    img = image_data[\"image\"]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    gt_landmarks = tf.expand_dims(image_data[\"landmarks_2d\"], 0)\n",
    "    gt_boxes = generate_bboxes_from_landmarks(gt_landmarks)\n",
    "    gt_landmarks = filter_landmarks(gt_landmarks)\n",
    "    img = tf.image.resize(img, (final_height, final_width))\n",
    "    if augmentation_fn:\n",
    "        img, gt_boxes, gt_landmarks = augmentation_fn(img, gt_boxes,\n",
    "                                                      gt_landmarks)\n",
    "    img = (img - 0.5) / 0.5\n",
    "    return img, gt_boxes, gt_landmarks\n",
    "\n",
    "\n",
    "def calculate_actual_outputs(prior_boxes, gt_boxes, gt_landmarks,\n",
    "                             hyper_params):\n",
    "    \"\"\"Calculate ssd actual output values.\n",
    "    Batch operations supported.\n",
    "    inputs:\n",
    "        prior_boxes = (total_bboxes, [center_x, center_y, width, height])\n",
    "            these values in normalized format between [0, 1]\n",
    "        gt_boxes = (batch_size, gt_box_size, [y1, x1, y2, x2])\n",
    "            these values in normalized format between [0, 1]\n",
    "        gt_landmarks = (batch_size, gt_box_size, total_landmarks, [x, y])\n",
    "            these values in normalized format between [0, 1]\n",
    "        hyper_params = dictionary\n",
    "\n",
    "    outputs:\n",
    "        actual_deltas = (batch_size, total_bboxes, [delta_bbox_y, delta_bbox_x,\n",
    "        delta_bbox_h, delta_bbox_w, delta_landmark_x0, delta_landmark_y0, ...,\n",
    "        delta_landmark_xN, delta_landmark_yN])\n",
    "        actual_labels = (batch_size, total_bboxes, [1 or 0])\n",
    "    \"\"\"\n",
    "\n",
    "    prior_boxes = tf.cast(prior_boxes, tf.float32)  # ensure all float values\n",
    "\n",
    "    batch_size = tf.shape(gt_boxes)[0]\n",
    "    iou_threshold = hyper_params[\"iou_threshold\"]\n",
    "    variances = hyper_params[\"variances\"]\n",
    "    total_landmarks = hyper_params[\"total_landmarks\"]\n",
    "    landmark_variances = total_landmarks * variances[0:2]\n",
    "\n",
    "    # Calculate intersection (jaccard index) between each bboxes and\n",
    "    # ground truth boxes\n",
    "    prior_box_corners = convert_xywh_to_bboxes(prior_boxes)\n",
    "    iou_map = generate_iou_map(prior_box_corners, gt_boxes)\n",
    "\n",
    "    # Get max index value for each row\n",
    "    max_indices_each_gt_box = tf.argmax(iou_map, axis=2, output_type=tf.int32)\n",
    "    # IoU map has iou values for every gt boxes and we merge these values\n",
    "    # column wise\n",
    "    merged_iou_map = tf.reduce_max(iou_map, axis=2)\n",
    "\n",
    "    pos_cond = tf.greater(merged_iou_map, iou_threshold)\n",
    "\n",
    "    gt_landmarks = tf.reshape(gt_landmarks,\n",
    "                              (batch_size, -1, total_landmarks * 2))\n",
    "    gt_boxes_and_landmarks = tf.concat([gt_boxes, gt_landmarks], -1)\n",
    "    gt_boxes_and_landmarks_map = tf.gather(\n",
    "        gt_boxes_and_landmarks, max_indices_each_gt_box, batch_dims=1\n",
    "    )\n",
    "    expanded_gt_boxes_and_landmarks = tf.where(\n",
    "        tf.expand_dims(pos_cond, -1),\n",
    "        gt_boxes_and_landmarks_map,\n",
    "        tf.zeros_like(gt_boxes_and_landmarks_map),\n",
    "    )\n",
    "    actual_deltas = get_deltas_from_bboxes_and_landmarks(\n",
    "        prior_boxes, expanded_gt_boxes_and_landmarks\n",
    "    ) / (variances + landmark_variances)\n",
    "\n",
    "    actual_labels = tf.expand_dims(tf.cast(pos_cond, dtype=tf.float32), -1)\n",
    "\n",
    "    return actual_deltas, actual_labels\n",
    "\n",
    "\n",
    "def generator_finite(dataset, prior_boxes, hyper_params):\n",
    "    for image_data in dataset:\n",
    "        img, gt_boxes, gt_landmarks = image_data\n",
    "        actual_deltas, actual_labels = calculate_actual_outputs(\n",
    "            prior_boxes, gt_boxes, gt_landmarks, hyper_params\n",
    "        )\n",
    "        yield img, (actual_deltas, actual_labels)\n",
    "\n",
    "\n",
    "def generator_infinite(dataset, prior_boxes, hyper_params):\n",
    "    while True:\n",
    "        for image_data in dataset:\n",
    "            img, gt_boxes, gt_landmarks = image_data\n",
    "            actual_deltas, actual_labels = calculate_actual_outputs(\n",
    "                prior_boxes, gt_boxes, gt_landmarks, hyper_params\n",
    "            )\n",
    "            yield img, (actual_deltas, actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a54e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ #\n",
    "# Prepare data #\n",
    "# ============ #\n",
    "\n",
    "print(\"Loading tfds dataset:\", config[\"tfds_dataset_name\"])\n",
    "train_split = \"train[:{}%]\".format(config[\"data_train_split_percentage\"])\n",
    "rest_split = \"train[{}%:]\".format(config[\"data_train_split_percentage\"])\n",
    "train_data, info = tfds.load(\n",
    "    config[\"tfds_dataset_name\"],\n",
    "    split=train_split,\n",
    "    data_dir=config[\"data_dir\"],\n",
    "    with_info=True,\n",
    ")\n",
    "rest_data, _ = tfds.load(\n",
    "    config[\"tfds_dataset_name\"],\n",
    "    split=rest_split,\n",
    "    data_dir=config[\"data_dir\"],\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "train_total_items = info.splits[train_split].num_examples\n",
    "rest_total_items = info.splits[rest_split].num_examples\n",
    "\n",
    "val_total_items = int(rest_total_items / 2)\n",
    "test_total_items = rest_total_items - val_total_items\n",
    "print(\"train_total_items:    {:>10,d}\".format(train_total_items))\n",
    "print(\"val_total_items:      {:>10,d}\".format(val_total_items))\n",
    "print(\"test_total_items:     {:>10,d}\".format(test_total_items))\n",
    "\n",
    "val_data = rest_data.take(val_total_items)\n",
    "test_data = rest_data.skip(val_total_items).take(test_total_items)\n",
    "\n",
    "img_size = config[\"input_size\"][0]\n",
    "\n",
    "# import augmentation\n",
    "train_data = train_data.map(lambda x: preprocessing(x, img_size, img_size,\n",
    "                                                    apply_augmentation))\n",
    "val_data = val_data.map(lambda x: preprocessing(x, img_size, img_size))\n",
    "test_data = test_data.map(lambda x: preprocessing(x, img_size, img_size))\n",
    "\n",
    "# (images, ground truth boxes, ground truth landmarks)\n",
    "data_shapes = ([None, None, None], [None, None], [None, None, None])\n",
    "\n",
    "# (images, ground truth boxes, ground truth landmarks)\n",
    "padding_values = (\n",
    "    tf.constant(0, tf.float32),\n",
    "    tf.constant(0, tf.float32),\n",
    "    tf.constant(0, tf.float32),\n",
    ")\n",
    "\n",
    "train_data = train_data.shuffle(config[\"batch_size\"] * 12).padded_batch(\n",
    "    config[\"batch_size\"], padded_shapes=data_shapes,\n",
    "    padding_values=padding_values\n",
    ")\n",
    "val_data = val_data.padded_batch(\n",
    "    config[\"batch_size\"], padded_shapes=data_shapes,\n",
    "    padding_values=padding_values\n",
    ")\n",
    "test_data = test_data.padded_batch(\n",
    "    1, padded_shapes=data_shapes, padding_values=padding_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ #\n",
    "# Create model #\n",
    "# ============ #\n",
    "\n",
    "\n",
    "class BlazeFace(tf.Module):\n",
    "    def __init__(self, config={}):\n",
    "        super(BlazeFace, self).__init__()\n",
    "        self.config = config\n",
    "        self.config[\"input_shape\"] = [\n",
    "            config[\"input_size\"][0],\n",
    "            config[\"input_size\"][1],\n",
    "            3,\n",
    "        ]\n",
    "        self.model = None\n",
    "\n",
    "    def single_blaze_block(\n",
    "        self, inputs, filters=24, kernel_size=5, strides=1, padding=\"same\"\n",
    "    ):\n",
    "        depthwise1 = tf.keras.layers.DepthwiseConv2D(\n",
    "            kernel_size=kernel_size, strides=strides, padding=padding\n",
    "        )(inputs)\n",
    "        conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=1, kernel_size=1, padding=padding, activation=None\n",
    "        )(depthwise1)\n",
    "        # residual connection\n",
    "        if strides == 2:\n",
    "            maxpool1 = tf.keras.layers.MaxPool2D(pool_size=2)(inputs)\n",
    "            conv2 = tf.keras.layers.Conv2D(\n",
    "                filters=filters, kernel_size=1, padding=padding\n",
    "            )(maxpool1)\n",
    "            output = tf.keras.layers.Add()([conv1, conv2])\n",
    "        else:\n",
    "            output = tf.keras.layers.Add()([conv1, inputs])\n",
    "        return tf.keras.layers.Activation(\"relu\")(output)\n",
    "\n",
    "    def double_blaze_block(\n",
    "        self, inputs, filters1=24, filters2=96, kernel_size=5, strides=1,\n",
    "        padding=\"same\"\n",
    "    ):\n",
    "        depthwise1 = tf.keras.layers.DepthwiseConv2D(\n",
    "            kernel_size=kernel_size, strides=strides, padding=padding\n",
    "        )(inputs)\n",
    "        conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=filters1, kernel_size=1, padding=padding, activation=\"relu\"\n",
    "        )(depthwise1)\n",
    "        depthwise2 = tf.keras.layers.DepthwiseConv2D(\n",
    "            kernel_size=kernel_size, strides=1, padding=padding\n",
    "        )(conv1)\n",
    "        conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=filters2, kernel_size=1, padding=padding\n",
    "        )(depthwise2)\n",
    "        # residual\n",
    "        if strides == 2:\n",
    "            maxpool1 = tf.keras.layers.MaxPool2D(pool_size=2)(inputs)\n",
    "            conv3 = tf.keras.layers.Conv2D(\n",
    "                filters=filters2, kernel_size=1, padding=padding\n",
    "            )(maxpool1)\n",
    "            output = tf.keras.layers.Add()([conv2, conv3])\n",
    "        else:\n",
    "            output = tf.keras.layers.Add()([conv2, inputs])\n",
    "        return tf.keras.layers.Activation(\"relu\")(output)\n",
    "\n",
    "    def get_feature_extractor(self):\n",
    "        # =========== #\n",
    "        # Input layer #\n",
    "        # =========== #\n",
    "        inputs = tf.keras.layers.Input(shape=self.config[\"input_shape\"])\n",
    "\n",
    "        # ======================= #\n",
    "        # First convolution layer #\n",
    "        # ======================= #\n",
    "        # input 128x128x3; kernel 5x5x3x24, stride 2\n",
    "        convolution1 = tf.keras.layers.Conv2D(\n",
    "            filters=24, kernel_size=5, strides=2, padding=\"same\",\n",
    "            activation=\"relu\"\n",
    "        )(inputs)\n",
    "\n",
    "        # ======================== #\n",
    "        # Single blaze block phase #\n",
    "        # ======================== #\n",
    "        # input 64x64x24; kernels 5x5x24x1; 1x1x24x24\n",
    "        single_blaze1 = self.single_blaze_block(convolution1, filters=24)\n",
    "        # input 64x64x24; kernels 5x5x24x1; 1x1x24x24\n",
    "        single_blaze2 = self.single_blaze_block(single_blaze1, filters=24)\n",
    "        # input 64x64x24; kernels 5x5x24x1, stride 2; 1x1x24x48\n",
    "        single_blaze3 = self.single_blaze_block(single_blaze2, filters=48,\n",
    "                                                strides=2)\n",
    "        # input 32x32x48; kernels 5x5x48x1; 1x1x48x48\n",
    "        single_blaze4 = self.single_blaze_block(single_blaze3, filters=48)\n",
    "        # input 32x32x48; kernels 5x5x48x1; 1x1x48x48\n",
    "        single_blaze5 = self.single_blaze_block(single_blaze4, filters=48)\n",
    "\n",
    "        # ======================== #\n",
    "        # Double blaze block phase #\n",
    "        # ======================== #\n",
    "        # input 32x32x48; kernels 5x5x48x1, stride 2; 1x1x48x24; 5x5x24x1;\n",
    "        # 1x1x24x96\n",
    "        double_blaze1 = self.double_blaze_block(\n",
    "            single_blaze5, filters1=24, filters2=96, strides=2\n",
    "        )\n",
    "        # input 16x16x96; kernels 5x5x96x1; 1x1x96x24; 5x5x24x1; 1x1x24x96\n",
    "        double_blaze2 = self.double_blaze_block(\n",
    "            double_blaze1, filters1=24, filters2=96, strides=1\n",
    "        )\n",
    "        # input 16x16x96; kernels 5x5x96x1; 1x1x96x24; 5x5x24x1; 1x1x24x96\n",
    "        double_blaze3 = self.double_blaze_block(\n",
    "            double_blaze2, filters1=24, filters2=96, strides=1\n",
    "        )\n",
    "        # input 16x16x96; kernels 5x5x96x1, stride 2; 1x1x96x24; 5x5x24x1;\n",
    "        # 1x1x24x96\n",
    "        double_blaze4 = self.double_blaze_block(\n",
    "            double_blaze3, filters1=24, filters2=96, strides=2\n",
    "        )\n",
    "        # input 8x8x96; kernels 5x5x96x1; 1x1x96x24; 5x5x24x1; 1x1x24x96\n",
    "        double_blaze5 = self.double_blaze_block(\n",
    "            double_blaze4, filters1=24, filters2=96, strides=1\n",
    "        )\n",
    "        # input 8x8x96; kernels 5x5x96x1; 1x1x96x24; 5x5x24x1; 1x1x24x96\n",
    "        double_blaze6 = self.double_blaze_block(\n",
    "            double_blaze5, filters1=24, filters2=96, strides=1\n",
    "        )\n",
    "\n",
    "        return Model(inputs=inputs, outputs=[double_blaze3, double_blaze6])\n",
    "\n",
    "    def build_model(self, compile=False):\n",
    "        feature_extractor = self.get_feature_extractor()\n",
    "\n",
    "        total_reg_points = self.config[\"total_landmarks\"] * 2 + 4\n",
    "\n",
    "        # ============== #\n",
    "        # Detection head #\n",
    "        # ============== #\n",
    "        # labels: 16x16, [batch, 16, 16, 6]\n",
    "        labels1 = tf.keras.layers.Conv2D(\n",
    "            filters=self.config[\"detections_per_layer\"][1] * 1,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "        )(feature_extractor.output[0])\n",
    "        labels1_reshaped = tf.keras.layers.Reshape(\n",
    "            (16**2 * self.config[\"detections_per_layer\"][1], 1)\n",
    "        )(labels1)\n",
    "        # labels: 8x8, [batch, 8, 8, 2]\n",
    "        labels2 = tf.keras.layers.Conv2D(\n",
    "            filters=self.config[\"detections_per_layer\"][0] * 1,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "        )(feature_extractor.output[1])\n",
    "        labels2_reshaped = tf.keras.layers.Reshape(\n",
    "            (8**2 * self.config[\"detections_per_layer\"][0], 1)\n",
    "        )(labels2)\n",
    "        # labels: [batch, 896, 1]\n",
    "        labels_combined = tf.keras.layers.Concatenate(name=\"conf\", axis=1)(\n",
    "            [labels1_reshaped, labels2_reshaped]\n",
    "        )\n",
    "\n",
    "        # boxes: 16x16, [x, y, w, h] # x6\n",
    "        boxes1 = tf.keras.layers.Conv2D(\n",
    "            filters=self.config[\"detections_per_layer\"][1] * total_reg_points,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "        )(feature_extractor.output[0])\n",
    "        boxes1_reshaped = tf.keras.layers.Reshape(\n",
    "            (16**2 * self.config[\"detections_per_layer\"][1], total_reg_points)\n",
    "        )(boxes1)\n",
    "        # boxes: 8x8, [x, y, w, h] # x2\n",
    "        boxes2 = tf.keras.layers.Conv2D(\n",
    "            self.config[\"detections_per_layer\"][0] * total_reg_points,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "        )(feature_extractor.output[1])\n",
    "        boxes2_reshaped = tf.keras.layers.Reshape(\n",
    "            (8**2 * self.config[\"detections_per_layer\"][0], total_reg_points)\n",
    "        )(boxes2)\n",
    "        # boxes: [batch, 896, 1]\n",
    "        boxes_combined = tf.keras.layers.Concatenate(name=\"loc\", axis=1)(\n",
    "            [boxes1_reshaped, boxes2_reshaped]\n",
    "        )\n",
    "\n",
    "        self.model = Model(\n",
    "            inputs=feature_extractor.input,\n",
    "            outputs=[boxes_combined, labels_combined]\n",
    "        )\n",
    "\n",
    "        self.init_model(self.model, compile=compile)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def init_model(self, model, compile=True):\n",
    "        # Compile model with loss function, and initialise with random data\n",
    "        if compile is True:\n",
    "            custom_losses = CustomLoss(\n",
    "                self.config[\"neg_pos_ratio\"], self.config[\"loc_loss_alpha\"]\n",
    "            )\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=self.get_learning_rate()),\n",
    "                loss=[custom_losses.loc_loss_fn, custom_losses.conf_loss_fn],\n",
    "            )\n",
    "\n",
    "        img_size = model.input.shape[1]\n",
    "        model(tf.random.uniform((1, img_size, img_size, 3)))\n",
    "\n",
    "    def get_learning_rate(self, epoch=1):\n",
    "        if epoch < 100:\n",
    "            learning_rate = 1e-3\n",
    "        elif epoch < 125:\n",
    "            learning_rate = 1e-4\n",
    "        else:\n",
    "            learning_rate = 1e-5\n",
    "        return learning_rate\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(\n",
    "                [None, config[\"input_size\"][0], config[\"input_size\"][0], 3],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"input_1\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    def predict(self, inputs):\n",
    "        prediction = self.model(inputs)\n",
    "        #         outputs = {'loc':  prediction[0],\n",
    "        #                    'conf': prediction[1]}\n",
    "        #         return {outputs}\n",
    "        return {\"loc\": prediction[0], \"conf\": prediction[1]}\n",
    "\n",
    "\n",
    "blazeface = BlazeFace(config)\n",
    "model = blazeface.build_model()\n",
    "print(\"model input:\", model.input)\n",
    "print(\"model output:\", model.output)\n",
    "print(\"model signature:\", blazeface.predict.get_concrete_function())\n",
    "if PLOT_MODEL:\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(\n",
    "        model, to_file=model_plot_file, show_shapes=True, show_layer_names=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728609fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== #\n",
    "# Save & export model #\n",
    "# =================== #\n",
    "\n",
    "concrete_function = blazeface.predict.get_concrete_function()\n",
    "\n",
    "if LOAD_SAVED_WEIGHTS:\n",
    "    print(\"Load saved weights: \", weights_file)\n",
    "    model.load_weights(weights_file)\n",
    "\n",
    "if EXPORT_SAVED_MODEL:\n",
    "    print(\"Save model: \", saved_model_path)\n",
    "    tf.saved_model.save(\n",
    "        blazeface,\n",
    "        saved_model_path,\n",
    "        signatures={\"predict\": concrete_function},\n",
    "    )\n",
    "\n",
    "if EXPORT_TFLITE_MODEL:\n",
    "    print(\"Save TFLite file:\", tflite_file)\n",
    "    #     converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    #         [concrete_function],\n",
    "    #         blazeface) # or model\n",
    "    #     converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "    #     converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(blazeface)  # or model\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # save TFLite file\n",
    "    with open(tflite_file, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Wrote TFLite model of %d bytes.\" % (len(tflite_model)))\n",
    "\n",
    "    # Print the signatures from the converted model\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    signatures = interpreter.get_signature_list()\n",
    "    print(\"signatures:\", signatures)\n",
    "\n",
    "if EXPORT_QUANTIZED_MODEL:\n",
    "    print(\"Save TFLite quantized file:\", tflite_quant_file)\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    # Quantization settings\n",
    "    num_calibration_examples = 100\n",
    "    representative_dataset = None\n",
    "\n",
    "    print(\n",
    "        \"- using {} test inputs to optimise quantization\".format(\n",
    "            num_calibration_examples\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def representative_dataset():\n",
    "        for image_data in test_data.take(num_calibration_examples):\n",
    "            img, gt_boxes, gt_landmarks = image_data\n",
    "            yield {\"input_1\": img}\n",
    "\n",
    "    #     converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    #         [concrete_function],\n",
    "    #         blazeface) # model\n",
    "    #     converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "    #     converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(blazeface)  # or model\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if representative_dataset:  # This is optional, see above.\n",
    "        converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(tflite_quant_file, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Wrote TFLite model of %d bytes.\" % (len(tflite_model)))\n",
    "\n",
    "    # Print the signatures from the converted model\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    signatures = interpreter.get_signature_list()\n",
    "    print(\"signatures:\", signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02290aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== #\n",
    "# Inference Test #\n",
    "# ============== #\n",
    "\n",
    "variances = blazeface.config[\"variances\"]\n",
    "total_landmarks = blazeface.config[\"total_landmarks\"]\n",
    "\n",
    "landmark_variances = total_landmarks * variances[0:2]\n",
    "total_variances = variances + landmark_variances\n",
    "\n",
    "\n",
    "def draw_bboxes(imgs, bboxes):\n",
    "    \"\"\"Drawing bounding boxes on given images.\n",
    "    inputs:\n",
    "        imgs = (batch_size, height, width, channels)\n",
    "        bboxes = (batch_size, total_bboxes, [y1, x1, y2, x2])\n",
    "            in normalized form [0, 1]\n",
    "    \"\"\"\n",
    "    colors = tf.constant([[1, 0, 0, 1]], dtype=tf.float32)\n",
    "    imgs_with_bb = tf.image.draw_bounding_boxes(imgs, bboxes, colors)\n",
    "    plt.figure()\n",
    "    for img_with_bb in imgs_with_bb:\n",
    "        plt.imshow(img_with_bb)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def local_draw_bboxes_with_landmarks(img, bboxes, landmarks, scores=None):\n",
    "    \"\"\"Drawing bounding boxes and landmarks on given image.\n",
    "    inputs:\n",
    "        img = (height, width, channels)\n",
    "        bboxes = (total_bboxes, [y1, x1, y2, x2])\n",
    "        landmarks = (total_bboxes, total_landmarks, [x, y])\n",
    "        scores = (totalbboxes, [scores])\n",
    "    \"\"\"\n",
    "    image = tf.keras.preprocessing.image.array_to_img(img)\n",
    "    width, height = image.size\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = (255, 0, 0, 255)\n",
    "    fnt = None\n",
    "    for index, bbox in enumerate(bboxes):\n",
    "        y1, x1, y2, x2 = tf.split(bbox, 4)\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        if width <= 0 or height <= 0:\n",
    "            continue\n",
    "        draw.rectangle((x1, y1, x2, y2), outline=color, width=1)\n",
    "        try:\n",
    "            score = scores[index]\n",
    "            draw.text((x1, y1 - 10), '{:.2f}'.format(score),\n",
    "                      font=fnt, fill=color)\n",
    "        except Exception:\n",
    "            pass\n",
    "    for index, landmark in enumerate(landmarks):\n",
    "        if tf.reduce_max(landmark) <= 0:\n",
    "            continue\n",
    "        rects = tf.concat([landmark - 1, landmark + 1], -1)\n",
    "        for rect in rects:\n",
    "            draw.ellipse(rect, fill=color)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def singleImageTensorToPil(single_img_tensor):\n",
    "    img_pil = tf.keras.preprocessing.image.array_to_img(single_img_tensor)\n",
    "    return img_pil\n",
    "\n",
    "\n",
    "# Inference tests\n",
    "for image_data in test_data.take(config[\"num_test_images\"]):\n",
    "\n",
    "    # Get test image\n",
    "    img, shapes, values = image_data\n",
    "\n",
    "    # Run inference\n",
    "    pred_deltas, pred_scores = model.predict_on_batch(img)\n",
    "\n",
    "    pred_deltas *= total_variances\n",
    "\n",
    "    pred_bboxes_and_landmarks = get_bboxes_and_landmarks_from_deltas(\n",
    "        prior_boxes, pred_deltas\n",
    "    )\n",
    "    pred_bboxes_and_landmarks = tf.clip_by_value(pred_bboxes_and_landmarks,\n",
    "                                                 0, 1)\n",
    "\n",
    "    pred_scores = tf.cast(pred_scores, tf.float32)\n",
    "\n",
    "    weighted_suppressed_data, filtered_scores = weighted_suppression(\n",
    "        pred_scores[0], pred_bboxes_and_landmarks[0]\n",
    "    )\n",
    "\n",
    "    weighted_bboxes = weighted_suppressed_data[..., 0:4]\n",
    "    weighted_landmarks = weighted_suppressed_data[..., 4:]\n",
    "\n",
    "    denormalized_bboxes = denormalize_bboxes(weighted_bboxes,\n",
    "                                             img_size, img_size)\n",
    "    weighted_landmarks = tf.reshape(weighted_landmarks,\n",
    "                                    (-1, total_landmarks, 2))\n",
    "    denormalized_landmarks = denormalize_landmarks(\n",
    "        weighted_landmarks, img_size, img_size\n",
    "    )\n",
    "\n",
    "    img_plot = local_draw_bboxes_with_landmarks(\n",
    "        img[0], denormalized_bboxes, denormalized_landmarks,\n",
    "        filtered_scores[..., 0]\n",
    "    )\n",
    "\n",
    "    bboxes_truth_abs = denormalize_bboxes(shapes[0], img_size, img_size)\n",
    "    landmarks_truth_abs = denormalize_landmarks(values[0], img_size, img_size)\n",
    "    img_plot_truth = local_draw_bboxes_with_landmarks(\n",
    "        img[0], bboxes_truth_abs, landmarks_truth_abs\n",
    "    )\n",
    "\n",
    "    width = 10\n",
    "    height = 10\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(width, height))\n",
    "\n",
    "    axarr[0].set_title(\"detection\")\n",
    "    axarr[0].imshow(img_plot)\n",
    "\n",
    "    axarr[1].set_title(\"ground truth\")\n",
    "    axarr[1].imshow(img_plot_truth)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bed09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFLite inference test #\n",
    "\n",
    "# Load the TFLite model in TFLite Interpreter\n",
    "print(\"Loading TFLite file: \", tflite_file)\n",
    "interpreter = tf.lite.Interpreter(tflite_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# check the type of the input tensor\n",
    "floating_model = interpreter.get_input_details()[0][\"dtype\"] == np.float32\n",
    "\n",
    "USE_SIGNATURE_FN = False\n",
    "\n",
    "if USE_SIGNATURE_FN:\n",
    "    # something in this next section causes problems for inference\n",
    "    signatures = interpreter.get_signature_list()\n",
    "    print(\"signatures:\", signatures)\n",
    "    sig_key = list(signatures.keys())[0]\n",
    "    # print('signature key:', sig_key)\n",
    "    my_signature_fn = interpreter.get_signature_runner(sig_key)\n",
    "else:\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_index = input_details[\"index\"]\n",
    "    output0_index = output_details[0][\"index\"]\n",
    "    output1_index = output_details[1][\"index\"]\n",
    "\n",
    "# Inference tests\n",
    "for image_data in test_data.take(config[\"num_test_images\"]):\n",
    "\n",
    "    # Get test image\n",
    "    img, shapes, values = image_data\n",
    "\n",
    "    # Run inference\n",
    "    pred_deltas, pred_scores = model.predict_on_batch(img)\n",
    "\n",
    "    pred_deltas *= total_variances\n",
    "\n",
    "    pred_bboxes_and_landmarks = get_bboxes_and_landmarks_from_deltas(\n",
    "        prior_boxes, pred_deltas\n",
    "    )\n",
    "    pred_bboxes_and_landmarks = tf.clip_by_value(pred_bboxes_and_landmarks,\n",
    "                                                 0, 1)\n",
    "\n",
    "    pred_scores = tf.cast(pred_scores, tf.float32)\n",
    "\n",
    "    weighted_suppressed_data, filtered_scores = weighted_suppression(\n",
    "        pred_scores[0], pred_bboxes_and_landmarks[0]\n",
    "    )\n",
    "\n",
    "    weighted_bboxes = weighted_suppressed_data[..., 0:4]\n",
    "    weighted_landmarks = weighted_suppressed_data[..., 4:]\n",
    "\n",
    "    denormalized_bboxes = denormalize_bboxes(weighted_bboxes,\n",
    "                                             img_size, img_size)\n",
    "    weighted_landmarks = tf.reshape(weighted_landmarks,\n",
    "                                    (-1, total_landmarks, 2))\n",
    "    denormalized_landmarks = denormalize_landmarks(\n",
    "        weighted_landmarks, img_size, img_size\n",
    "    )\n",
    "\n",
    "    img_plot = local_draw_bboxes_with_landmarks(\n",
    "        img[0], denormalized_bboxes, denormalized_landmarks,\n",
    "        filtered_scores[..., 0]\n",
    "    )\n",
    "\n",
    "    bboxes_truth_abs = denormalize_bboxes(shapes[0], img_size, img_size)\n",
    "    landmarks_truth_abs = denormalize_landmarks(values[0], img_size, img_size)\n",
    "    img_plot_truth = local_draw_bboxes_with_landmarks(\n",
    "        img[0], bboxes_truth_abs, landmarks_truth_abs\n",
    "    )\n",
    "\n",
    "    width = 10\n",
    "    height = 10\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(width, height))\n",
    "\n",
    "    axarr[0].set_title(\"detection\")\n",
    "    axarr[0].imshow(img_plot)\n",
    "\n",
    "    axarr[1].set_title(\"ground truth\")\n",
    "    axarr[1].imshow(img_plot_truth)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4208e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
